-- Generated by Ora2Pg, the Oracle database Schema converter, version 21.1
-- Copyright 2000-2020 Gilles DAROLD. All rights reserved.
-- DATASOURCE: dbi:Oracle:INT8R

SET client_encoding TO 'UTF8';

\set ON_ERROR_STOP ON

SET check_function_bodies = false;

CREATE SCHEMA IF NOT EXISTS doma_panda;
ALTER SCHEMA doma_panda OWNER TO panda;

SET search_path = doma_panda,public;

CREATE TABLE cache (
	main_key varchar(56) NOT NULL,
	sub_key varchar(56) NOT NULL,
	data text,
	last_update timestamp
) ;
COMMENT ON TABLE cache IS E'Table to store arbitrary CLOBs';
COMMENT ON COLUMN cache.data IS E'Information to cache';
COMMENT ON COLUMN cache.last_update IS E'Timestamp value was last updated';
COMMENT ON COLUMN cache.main_key IS E'Main key';
COMMENT ON COLUMN cache.sub_key IS E'Sub key';
ALTER TABLE cache OWNER TO panda;
ALTER TABLE cache ADD PRIMARY KEY (main_key,sub_key);

CREATE TABLE cloudtasks (
	id integer NOT NULL,
	taskname varchar(128),
	taskid integer,
	cloud varchar(20),
	status varchar(20),
	tmod timestamp NOT NULL DEFAULT LOCALTIMESTAMP,
	tenter timestamp NOT NULL DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss')
) ;
COMMENT ON TABLE cloudtasks IS E'Table for task brokerage which assigns production tasks to clouds by checking data locality and work distribution';
COMMENT ON COLUMN cloudtasks.cloud IS E'the cloud name where the task is assigned';
COMMENT ON COLUMN cloudtasks.id IS E'autoincremented id of the row generated from ATLAS_PANDA.CLOUDTASKS_ID_SEQ';
COMMENT ON COLUMN cloudtasks.status IS E'status of the brokerage procedure';
COMMENT ON COLUMN cloudtasks.taskid IS E'task identifier comes from etask.taskid';
COMMENT ON COLUMN cloudtasks.taskname IS E'the name of the task';
COMMENT ON COLUMN cloudtasks.tenter IS E'set when the task is inserted';
COMMENT ON COLUMN cloudtasks.tmod IS E'updated when status is changed';
ALTER TABLE cloudtasks OWNER TO panda;
CREATE INDEX cloudtasks_task_idx ON cloudtasks (taskname, taskid);
ALTER TABLE cloudtasks ADD PRIMARY KEY (id);

CREATE UNLOGGED TABLE "cmp3$19993173" (
	pandaid bigint,
	modificationtime timestamp NOT NULL,
	metadata text
) ;
ALTER TABLE "cmp3$19993173" OWNER TO panda;

CREATE TABLE config (
	app varchar(64) NOT NULL,
	component varchar(64) NOT NULL,
	key varchar(64) NOT NULL,
	value varchar(256) NOT NULL,
	type varchar(64) NOT NULL,
	vo varchar(16) NOT NULL,
	descr varchar(256) NOT NULL
) ;
COMMENT ON TABLE config IS E'Central configuration table for jedi and panda server';
COMMENT ON COLUMN config.app IS E'Application. E.g. jedi or pandaserver';
COMMENT ON COLUMN config.component IS E'Component. E.g. prodjobbrokerage';
COMMENT ON COLUMN config.descr IS E'Description what the entry is used for';
COMMENT ON COLUMN config.key IS E'Key for config entry. E.g. iointensitylimit';
COMMENT ON COLUMN config.type IS E'Python type, e.g. bool, int...';
COMMENT ON COLUMN config.value IS E'Value for config entry. E.g. 500';
COMMENT ON COLUMN config.vo IS E'VO/Experiment, e.g. atlas, ams, compass...';
ALTER TABLE config OWNER TO panda;
ALTER TABLE config ADD PRIMARY KEY (app,component,key,vo);

CREATE TABLE datasets (
	vuid varchar(40) NOT NULL,
	name varchar(255) NOT NULL,
	version varchar(10),
	type varchar(20),
	status varchar(10),
	numberfiles integer,
	currentfiles integer,
	creationdate timestamp,
	modificationdate timestamp NOT NULL,
	moverid bigint NOT NULL DEFAULT 0,
	transferstatus smallint NOT NULL DEFAULT 0,
	subtype varchar(5)
) PARTITION BY RANGE (modificationdate) ;
COMMENT ON TABLE datasets IS E'Datasets with which the PanDA jobs work with for a certain period. Data retention is defined to be 3 months (can be changed if necessary)';
COMMENT ON COLUMN datasets.creationdate IS E'set when the dataset is used in panda at the first time';
COMMENT ON COLUMN datasets.currentfiles IS E'current number of files in the dataset';
COMMENT ON COLUMN datasets.modificationdate IS E'updated when contents and/or status of the dataset is changed (in UTC)';
COMMENT ON COLUMN datasets.moverid IS E'PandaID of the pandamover which transfers files in the dataset';
COMMENT ON COLUMN datasets.name IS E'Dataset name. The _sub+number comes from auto-incremented value of the SUBCOUNTER_SUBID_SEQ sequence. It is used to have a unique number in each panda internal dataset name. The Sequence is defined to be cycling from values 1 to 9999999 in order to restrict the number of digits to maximum 7';
COMMENT ON COLUMN datasets.numberfiles IS E'the total number of files in the dataset';
COMMENT ON COLUMN datasets.status IS E'dataset status';
COMMENT ON COLUMN datasets.subtype IS E'sub-type of the dataset';
COMMENT ON COLUMN datasets.transferstatus IS E'used when the dataset is transferred to multiple destinations';
COMMENT ON COLUMN datasets.type IS E'type of the dataset, such as input, output, and log';
COMMENT ON COLUMN datasets.version IS E'version number';
COMMENT ON COLUMN datasets.vuid IS E'vuid of the dataset';
ALTER TABLE datasets OWNER TO panda;
CREATE INDEX datasets_modifdate_idx ON datasets (modificationdate);
CREATE INDEX datasets_moverid_indx ON datasets (moverid);
CREATE INDEX datasets_name_idx ON datasets (name);
CREATE INDEX datasets_stat_type_mdate_idx ON datasets (type, status, modificationdate, subtype);
ALTER TABLE datasets ADD PRIMARY KEY (vuid,modificationdate);

CREATE TABLE ddm_endpoint (
	ddm_endpoint_name varchar(52) NOT NULL,
	site_name varchar(52),
	ddm_spacetoken_name varchar(52),
	space_total bigint,
	space_free bigint,
	space_used bigint,
	is_tape varchar(1),
	type varchar(20),
	blacklisted char(1),
	space_expired bigint,
	space_timestamp timestamp,
	blacklisted_read char(1),
	blacklisted_write char(1)
) ;
COMMENT ON TABLE ddm_endpoint IS E'DDM/Rucio storage endpoint';
COMMENT ON COLUMN ddm_endpoint.blacklisted IS E'Defines whether a DDM endpoint is blacklisted or not (Y/N)';
COMMENT ON COLUMN ddm_endpoint.ddm_endpoint_name IS E'DDM endpoint name';
COMMENT ON COLUMN ddm_endpoint.ddm_spacetoken_name IS E'Spacetoken name';
COMMENT ON COLUMN ddm_endpoint.is_tape IS E'Defines whether the DDM endpoint is a tape storageY/N';
COMMENT ON COLUMN ddm_endpoint.site_name IS E'Site name';
COMMENT ON COLUMN ddm_endpoint.space_expired IS E'Expired, used space in GB';
COMMENT ON COLUMN ddm_endpoint.space_free IS E'Free space of a DDM endpoint as reported by Rucio. Value in GB';
COMMENT ON COLUMN ddm_endpoint.space_timestamp IS E'Timestamp reported by Rucio for the SRM space values';
COMMENT ON COLUMN ddm_endpoint.space_total IS E'Total space of a DDM endpoint as reported by Rucio. Value in GB';
COMMENT ON COLUMN ddm_endpoint.space_used IS E'Used space of a DDM endpoint as reported by Rucio. Value in GB';
COMMENT ON COLUMN ddm_endpoint.type IS E'Type of spacetoken, e.g. DATADISK, LOCALGROUPDISK...';
ALTER TABLE ddm_endpoint OWNER TO panda;
CREATE INDEX ddm_endpoint_site_name_idx ON ddm_endpoint (site_name);
ALTER TABLE ddm_endpoint ADD PRIMARY KEY (ddm_endpoint_name);
ALTER TABLE ddm_endpoint ADD CONSTRAINT blacklisted_check CHECK ( blacklisted IN ('Y', 'N'));

CREATE TABLE filestable4 (
	row_id bigint NOT NULL,
	pandaid bigint NOT NULL DEFAULT '0',
	modificationtime timestamp NOT NULL DEFAULT to_date('01-JAN-1970 00:00:00','DD-MON-YYYY HH24:MI:SS'),
	guid varchar(64),
	lfn varchar(256),
	type varchar(20),
	dataset varchar(255),
	status varchar(64),
	proddblock varchar(255),
	proddblocktoken varchar(250),
	dispatchdblock varchar(255),
	dispatchdblocktoken varchar(250),
	destinationdblock varchar(255),
	destinationdblocktoken varchar(250),
	destinationse varchar(250),
	fsize bigint NOT NULL DEFAULT '0',
	md5sum varchar(36),
	checksum varchar(36),
	scope varchar(30),
	jeditaskid bigint,
	datasetid bigint,
	fileid bigint,
	attemptnr smallint
) PARTITION BY RANGE (modificationtime) ;
COMMENT ON TABLE filestable4 IS E'Table for hosting the files each PanDA job deals with (input, output, log). When a PanDA job is in a defined or running state, relevant rows reside in the INITIAL partition of the table. When it is finished or aborted the "modificationtime" is set to the real current time and as the table has "row movement" enabled, Oracle moves the rows from the INITIAL partition to the partitions of the current day. Data is regularly copied to an archive table in ATLAS_PANDAARCH schema. Data retention of the FILESTABLE4 table is defined to be 30 days (can be changed if necessary)';
COMMENT ON COLUMN filestable4.checksum IS E'any checksum of the file (either adler32 or md5sum)';
COMMENT ON COLUMN filestable4.dataset IS E'dataset name where input/output files belong to';
COMMENT ON COLUMN filestable4.destinationdblock IS E'name of destination datablock is used to register the outputs of an associated set of jobs as belonging to one block to be saved at an archival destination';
COMMENT ON COLUMN filestable4.destinationdblocktoken IS E'token of DESTINATIONDBLOCK';
COMMENT ON COLUMN filestable4.destinationse IS E'destination storage element (archival destination) of output files';
COMMENT ON COLUMN filestable4.dispatchdblock IS E'name of dispatch datablock; a prodDBlock may be broken down into smaller blocks for dispatch to sites';
COMMENT ON COLUMN filestable4.dispatchdblocktoken IS E'token of DISPATCHDBLOCK';
COMMENT ON COLUMN filestable4.fsize IS E'file size in bytes';
COMMENT ON COLUMN filestable4.guid IS E'GUID of the file';
COMMENT ON COLUMN filestable4.lfn IS E'logical file name of the file';
COMMENT ON COLUMN filestable4.md5sum IS E'md5sum of the file';
COMMENT ON COLUMN filestable4.modificationtime IS E'modification time of the job (in UTC)';
COMMENT ON COLUMN filestable4.pandaid IS E'PandaID of the job';
COMMENT ON COLUMN filestable4.proddblock IS E'name of datablock where input files of the job is part of';
COMMENT ON COLUMN filestable4.proddblocktoken IS E'token of PRODDBLOCK';
COMMENT ON COLUMN filestable4.row_id IS E'auto-increment ID of the row generated from the FILESTABLE4_ROW_ID_SEQ sequence';
COMMENT ON COLUMN filestable4.scope IS E'Scope (user, group, project) of the file provided by the DDM system';
COMMENT ON COLUMN filestable4.status IS E'status of the file';
COMMENT ON COLUMN filestable4.type IS E'type of the file';
ALTER TABLE filestable4 OWNER TO panda;
CREATE INDEX filestable4_datasetype3col_idx ON filestable4 (dataset, type, destinationdblock, status, pandaid);
CREATE INDEX filestable4_destdblock_idx ON filestable4 (destinationdblock);
CREATE INDEX filestable4_dispdblock_idx ON filestable4 (dispatchdblock);
CREATE INDEX filestable4_lfn_idx ON filestable4 (lfn);
CREATE INDEX filestable4_pandaid_idx ON filestable4 (pandaid);
CREATE INDEX filestable4_taskfileid_idx ON filestable4 (jeditaskid, datasetid, fileid);
ALTER TABLE filestable4 ADD PRIMARY KEY (row_id,modificationtime);

CREATE TABLE global_shares (
	name varchar(32),
	value smallint NOT NULL,
	parent varchar(32),
	prodsourcelabel varchar(100),
	workinggroup varchar(100),
	campaign varchar(100),
	processingtype varchar(100),
	vo varchar(32),
	queue_id integer,
	throttled char(1),
	transpath varchar(128),
	rtype varchar(16)
) ;
COMMENT ON TABLE global_shares IS E'Global share definitions';
COMMENT ON COLUMN global_shares.campaign IS E'Campaign as in the jedi_tasks table. Can be python regexp';
COMMENT ON COLUMN global_shares.name IS E'Global share name';
COMMENT ON COLUMN global_shares.parent IS E'Name of the parent of the global share';
COMMENT ON COLUMN global_shares.processingtype IS E'Processing type as in the jedi_tasks table. Can be python regexp';
COMMENT ON COLUMN global_shares.prodsourcelabel IS E'Prod source label (user, managed, etc), as in the jedi_tasks table. Can be python regexp';
COMMENT ON COLUMN global_shares.queue_id IS E'Queue ID for compatibility reasons. Define the 9000 range as gs range, in order not to conflict with jedi_work_queue IDs';
COMMENT ON COLUMN global_shares.rtype IS E'Type of the resources, for example grid, cloud or hpc';
COMMENT ON COLUMN global_shares.throttled IS E'Global share is throttled or not';
COMMENT ON COLUMN global_shares.transpath IS E'The name of the transformation';
COMMENT ON COLUMN global_shares.value IS E'Percentage of resources assigned to share. E.g. 20';
COMMENT ON COLUMN global_shares.vo IS E'Virtual Organization';
COMMENT ON COLUMN global_shares.workinggroup IS E'Working group as in the jedi_tasks table. Can be python regexp';
ALTER TABLE global_shares OWNER TO panda;
CREATE UNIQUE INDEX global_shares_queue_id_uq ON global_shares (queue_id);

CREATE TABLE harvester_commands (
	command_id bigint NOT NULL,
	command varchar(200),
	harvester_id varchar(50),
	ack_requested smallint,
	creation_date timestamp,
	status varchar(32),
	status_date timestamp,
	params text
) ;
COMMENT ON TABLE harvester_commands IS E'Command queue for harvester';
COMMENT ON COLUMN harvester_commands.ack_requested IS E'0/1, depending on whether panda server expects and acknowledgement';
COMMENT ON COLUMN harvester_commands.command_id IS E'Command ID for primary key';
COMMENT ON COLUMN harvester_commands.creation_date IS E'Timestamp when the command was generated';
COMMENT ON COLUMN harvester_commands.params IS E'Parameters for the call';
COMMENT ON COLUMN harvester_commands.status IS E'Status of the call: new, retrieved, acknowledged...';
COMMENT ON COLUMN harvester_commands.status_date IS E'Timestamp when the status last changed';
ALTER TABLE harvester_commands OWNER TO panda;
ALTER TABLE harvester_commands ADD PRIMARY KEY (command_id);
ALTER TABLE harvester_commands ADD CONSTRAINT harvester_commands_ack_req_chk CHECK (ack_requested IN (0,1));

CREATE TABLE harvester_command_lock (
	harvester_id varchar(50) NOT NULL,
	computingsite varchar(128) NOT NULL,
	resourcetype varchar(56) NOT NULL,
	command varchar(200) NOT NULL,
	lockedtime timestamp,
	lockedby varchar(40)
) ;
COMMENT ON TABLE harvester_command_lock IS E'Exclusive locks to send commands to harvester';
COMMENT ON COLUMN harvester_command_lock.command IS E'Command string';
COMMENT ON COLUMN harvester_command_lock.computingsite IS E'Panda Queue name';
COMMENT ON COLUMN harvester_command_lock.harvester_id IS E'Identifier of the target harvester instance';
COMMENT ON COLUMN harvester_command_lock.lockedby IS E'Process name which locks the command';
COMMENT ON COLUMN harvester_command_lock.lockedtime IS E'Timestamp when the command is locked';
COMMENT ON COLUMN harvester_command_lock.resourcetype IS E'Resource type';
ALTER TABLE harvester_command_lock OWNER TO panda;
ALTER TABLE harvester_command_lock ADD PRIMARY KEY (harvester_id,computingsite,resourcetype,command);

CREATE TABLE harvester_dialogs (
	harvester_id varchar(50) NOT NULL,
	diagid bigint NOT NULL,
	modulename varchar(100),
	identifier varchar(100),
	creationtime timestamp,
	messagelevel varchar(10),
	diagmessage varchar(500)
) PARTITION BY RANGE (creationtime) ;
COMMENT ON TABLE harvester_dialogs IS E'Table for publishing diagnostic information of harvester instances in PandaMon';
COMMENT ON COLUMN harvester_dialogs.creationtime IS E'Timestamp when the message is sent';
COMMENT ON COLUMN harvester_dialogs.diagid IS E'Serial number of the message';
COMMENT ON COLUMN harvester_dialogs.diagmessage IS E'Dialog message';
COMMENT ON COLUMN harvester_dialogs.harvester_id IS E'Identifier of the harvester instance';
COMMENT ON COLUMN harvester_dialogs.identifier IS E'Identifier of the message if any';
COMMENT ON COLUMN harvester_dialogs.messagelevel IS E'Message level';
COMMENT ON COLUMN harvester_dialogs.modulename IS E'The module name which sent the message';
ALTER TABLE harvester_dialogs OWNER TO panda;
CREATE INDEX harv_dialogs_harv_diagid_idx ON harvester_dialogs (harvester_id, diagid);
CREATE INDEX harv_dialogs_module_harv_idx ON harvester_dialogs (modulename, harvester_id);

CREATE TABLE harvester_instances (
	harvester_id varchar(50) NOT NULL,
	description varchar(200),
	starttime timestamp,
	owner varchar(100),
	hostname varchar(100),
	lastupdate timestamp,
	sw_version varchar(50),
	commit_stamp varchar(100)
) ;
COMMENT ON TABLE harvester_instances IS E'Harvester instance registry';
COMMENT ON COLUMN harvester_instances.commit_stamp IS E'Commit stamp in the repository';
COMMENT ON COLUMN harvester_instances.description IS E'Harvester instance description';
COMMENT ON COLUMN harvester_instances.harvester_id IS E'Name of the harvester instance';
COMMENT ON COLUMN harvester_instances.hostname IS E'Hostname where the instance is running';
COMMENT ON COLUMN harvester_instances.lastupdate IS E'Data and time set when the record is updated';
COMMENT ON COLUMN harvester_instances.owner IS E'Owner of the instance';
COMMENT ON COLUMN harvester_instances.starttime IS E'Start time';
COMMENT ON COLUMN harvester_instances.sw_version IS E'Software version';
ALTER TABLE harvester_instances OWNER TO panda;
ALTER TABLE harvester_instances ADD PRIMARY KEY (harvester_id);

CREATE TABLE harvester_metrics (
	harvester_id varchar(50) NOT NULL,
	creation_time timestamp DEFAULT LOCALTIMESTAMP,
	harvester_host varchar(100),
	metrics varchar(4000)
) PARTITION BY RANGE (creation_time) ;
COMMENT ON TABLE harvester_metrics IS E'Table to store harvester service metrics. The metrics are in JSON dictionary, not very big (O(10) values like memory, CPU, etc.). Enforce data sliding window of N days (e.g. 30 days)';
COMMENT ON COLUMN harvester_metrics.metrics IS E'The harvester service metrics are in JSON structure which is in the order of tens key-value pairs (memory, CPU, etc.)';
ALTER TABLE harvester_metrics OWNER TO panda;
CREATE INDEX harvester_metrics_idx ON harvester_metrics (harvester_id, creation_time, harvester_host);

CREATE TABLE harvester_rel_jobs_workers (
	harvesterid varchar(50) NOT NULL,
	workerid bigint NOT NULL,
	pandaid bigint NOT NULL,
	lastupdate timestamp NOT NULL
) PARTITION BY RANGE (pandaid) ;
COMMENT ON TABLE harvester_rel_jobs_workers IS E'The table for relationship between jobs and workers. Combination of INSTANCEID, WORKERID, and PANDAID is unique. Deletion policy is to delete all records with LASTUPDATE<NOW-N_days';
COMMENT ON COLUMN harvester_rel_jobs_workers.harvesterid IS E'Identifier of the harvester instance';
COMMENT ON COLUMN harvester_rel_jobs_workers.lastupdate IS E'Set when the record is updated';
COMMENT ON COLUMN harvester_rel_jobs_workers.pandaid IS E'Job ID in PanDA';
COMMENT ON COLUMN harvester_rel_jobs_workers.workerid IS E'Identifier of the worker';
ALTER TABLE harvester_rel_jobs_workers OWNER TO panda;
CREATE INDEX harvrel_jobworkers_pandaid_idx ON harvester_rel_jobs_workers (pandaid);
CREATE INDEX harv_work_reljobs_lastupd_idx ON harvester_rel_jobs_workers (lastupdate);
ALTER TABLE harvester_rel_jobs_workers ADD PRIMARY KEY (harvesterid,workerid,pandaid);

CREATE TABLE harvester_slots (
	pandaqueuename varchar(128) NOT NULL,
	gshare varchar(32),
	resourcetype varchar(56),
	numslots bigint NOT NULL,
	modificationtime timestamp NOT NULL,
	expirationtime timestamp
) ;
COMMENT ON TABLE harvester_slots IS E'Table for workload provisioning. It is required for some special resources like Sim@P1 where PanDa needs to assign jobs before the resource becomes available. The number of records would be less than 100. The combination ofÂ PANDAQUEUENAME,Â GSHARE, andÂ RESOURCETYPE is unique. The table has to have UNIQUE key in order DB to allow NULL values for GSHARE, andÂ RESOURCETYPE';
COMMENT ON COLUMN harvester_slots.expirationtime IS E'The time when the record expires';
COMMENT ON COLUMN harvester_slots.gshare IS E'Global share';
COMMENT ON COLUMN harvester_slots.modificationtime IS E'The time when the record was updated';
COMMENT ON COLUMN harvester_slots.numslots IS E'The number of slots';
COMMENT ON COLUMN harvester_slots.pandaqueuename IS E'PanDA queue name';
COMMENT ON COLUMN harvester_slots.resourcetype IS E'Resource type';
ALTER TABLE harvester_slots OWNER TO panda;
ALTER TABLE harvester_slots ADD UNIQUE (pandaqueuename,gshare,resourcetype);

CREATE TABLE harvester_workers (
	harvesterid varchar(50) NOT NULL,
	workerid bigint NOT NULL,
	lastupdate timestamp NOT NULL,
	status varchar(80) NOT NULL,
	batchid varchar(80),
	nodeid varchar(80),
	queuename varchar(80),
	computingsite varchar(128),
	submittime timestamp,
	starttime timestamp,
	endtime timestamp,
	ncore integer,
	errorcode integer,
	stdout varchar(250),
	stderr varchar(250),
	batchlog varchar(250),
	resourcetype varchar(56),
	nativeexitcode integer,
	nativestatus varchar(80),
	diagmessage varchar(500),
	computingelement varchar(128),
	njobs integer,
	submissionhost varchar(128),
	harvesterhost varchar(128),
	jdl varchar(250),
	jobtype varchar(52)
) PARTITION BY RANGE (lastupdate) ;
COMMENT ON TABLE harvester_workers IS E'for workers submitted by harvesters. Combination of INSTANCEID and WORKERID is unique. Deletion policy is to delete all records with LASTUPDATE<NOW-N_days. To be enforced a data sliding window by partition removal although the PK has a global index (The ALTER TABLE ... DROP PARTITION ... UPDATE GLOBAL INDEXES)  to be used. ';
COMMENT ON COLUMN harvester_workers.batchid IS E'Unique ID in the batch system';
COMMENT ON COLUMN harvester_workers.batchlog IS E'URL for batch log';
COMMENT ON COLUMN harvester_workers.computingelement IS E'Gateway of the batch system';
COMMENT ON COLUMN harvester_workers.computingsite IS E'Panda Queue name';
COMMENT ON COLUMN harvester_workers.diagmessage IS E'Error diagnostics';
COMMENT ON COLUMN harvester_workers.endtime IS E'Set when the worker is terminated';
COMMENT ON COLUMN harvester_workers.errorcode IS E'Error code in any';
COMMENT ON COLUMN harvester_workers.harvesterhost IS E'The host name of the harvester node that generated the worker';
COMMENT ON COLUMN harvester_workers.harvesterid IS E'Identifier of the harvester instance';
COMMENT ON COLUMN harvester_workers.jdl IS E'Job definition sent to the CE';
COMMENT ON COLUMN harvester_workers.jobtype IS E'Type of job (prodSourceLabel) used for grand queue unification';
COMMENT ON COLUMN harvester_workers.lastupdate IS E'Set when the record is updated';
COMMENT ON COLUMN harvester_workers.nativeexitcode IS E'Exit code in the underlying system';
COMMENT ON COLUMN harvester_workers.nativestatus IS E'Status in the underlying system';
COMMENT ON COLUMN harvester_workers.ncore IS E'The number of cores the worker use';
COMMENT ON COLUMN harvester_workers.njobs IS E'The number of associated jobs';
COMMENT ON COLUMN harvester_workers.nodeid IS E'Identifier of the node, such as hostname, IP, etc';
COMMENT ON COLUMN harvester_workers.queuename IS E'Name of the batch queue';
COMMENT ON COLUMN harvester_workers.resourcetype IS E'Resource type';
COMMENT ON COLUMN harvester_workers.starttime IS E'Set when the worker gets CPUs';
COMMENT ON COLUMN harvester_workers.status IS E'Worker status';
COMMENT ON COLUMN harvester_workers.stderr IS E'URL for stderr';
COMMENT ON COLUMN harvester_workers.stdout IS E'URL for stdout';
COMMENT ON COLUMN harvester_workers.submissionhost IS E'The host name of the submission node';
COMMENT ON COLUMN harvester_workers.submittime IS E'Set when the worker is submitted';
COMMENT ON COLUMN harvester_workers.workerid IS E'Identifier of the worker';
ALTER TABLE harvester_workers OWNER TO panda;
CREATE INDEX harvester_workers_compsite_idx ON harvester_workers (computingsite);
CREATE INDEX harv_workers_harv_submtime_idx ON harvester_workers (harvesterid, submittime);
CREATE INDEX harv_workers_lastupd_idx ON harvester_workers (lastupdate);
CREATE INDEX harv_workers_submittime_idx ON harvester_workers (submittime);
ALTER TABLE harvester_workers ADD PRIMARY KEY (harvesterid,workerid,lastupdate);

CREATE TABLE harvester_worker_stats (
	harvester_id varchar(50) NOT NULL,
	computingsite varchar(128) NOT NULL,
	resourcetype varchar(56) NOT NULL,
	status varchar(80) NOT NULL,
	jobtype varchar(52) NOT NULL,
	n_workers integer,
	lastupdate timestamp NOT NULL
) ;
ALTER TABLE harvester_worker_stats OWNER TO panda;
ALTER TABLE harvester_worker_stats ADD PRIMARY KEY (harvester_id,computingsite,resourcetype,status,jobtype);

CREATE TABLE jedi_aux_status_mintaskid (
	status varchar(64) NOT NULL,
	min_jeditaskid bigint NOT NULL
) ;
ALTER TABLE jedi_aux_status_mintaskid OWNER TO panda;
ALTER TABLE jedi_aux_status_mintaskid ADD PRIMARY KEY (status);

CREATE TABLE jedi_datasets (
	jeditaskid bigint NOT NULL,
	datasetid bigint NOT NULL,
	datasetname varchar(255) NOT NULL,
	type varchar(20) NOT NULL,
	creationtime timestamp NOT NULL,
	modificationtime timestamp NOT NULL,
	vo varchar(16),
	cloud varchar(10),
	site varchar(60),
	masterid bigint,
	provenanceid bigint,
	containername varchar(255),
	status varchar(20),
	state varchar(20),
	statechecktime timestamp,
	statecheckexpiration timestamp,
	frozentime timestamp,
	nfiles bigint,
	nfilestobeused bigint,
	nfilesused bigint,
	nfilesonhold bigint,
	nevents bigint,
	neventstobeused bigint,
	neventsused bigint,
	lockedby varchar(40),
	lockedtime timestamp,
	nfilesfinished bigint,
	nfilesfailed bigint,
	attributes varchar(100),
	streamname varchar(20),
	storagetoken varchar(100),
	destination varchar(60),
	templateid bigint,
	nfileswaiting bigint
) PARTITION BY RANGE (jeditaskid) ;
COMMENT ON COLUMN jedi_datasets.attributes IS E'describes how the dataset is split. e,g, the ratio to the number of master files, no-split, repeat, etc';
COMMENT ON COLUMN jedi_datasets.cloud IS E'The replica in the cloud is used. Set manually or by the task brokerage';
COMMENT ON COLUMN jedi_datasets.containername IS E'The name of the container to which the dataset belongs. Set NULL when the dataset is not used as a part of a container';
COMMENT ON COLUMN jedi_datasets.creationtime IS E'Set when the record is inserted at the first time';
COMMENT ON COLUMN jedi_datasets.datasetid IS E'Auto-incremented ID of the dataset generated by the JEDI_DATASETS_ID_SEQ';
COMMENT ON COLUMN jedi_datasets.datasetname IS E'The dataset name';
COMMENT ON COLUMN jedi_datasets.destination IS E'The final destination where the dataset is transferred once the task is finished';
COMMENT ON COLUMN jedi_datasets.frozentime IS E'Set when the record is frozen';
COMMENT ON COLUMN jedi_datasets.jeditaskid IS E'The task identifier coming from the task table';
COMMENT ON COLUMN jedi_datasets.lockedby IS E'The name of process/thread which is taking care of the record';
COMMENT ON COLUMN jedi_datasets.lockedtime IS E'Set when the record is locked';
COMMENT ON COLUMN jedi_datasets.masterid IS E'Set the DATASETID of the master dataset if the dataset is used as a secondary dataset of the master. Otherwise, set NULL';
COMMENT ON COLUMN jedi_datasets.modificationtime IS E'Set when the record is updated';
COMMENT ON COLUMN jedi_datasets.nevents IS E'The total number of events in the dataset. Set NULL unless this info is required';
COMMENT ON COLUMN jedi_datasets.neventstobeused IS E'The number of events to be use. Set NULL unless this info is required';
COMMENT ON COLUMN jedi_datasets.neventsused IS E'The number of events used so far. Set NULL unless this info is required';
COMMENT ON COLUMN jedi_datasets.nfiles IS E'The total number of files in the dataset';
COMMENT ON COLUMN jedi_datasets.nfilesfailed IS E'The number of files failed so far';
COMMENT ON COLUMN jedi_datasets.nfilesfinished IS E'The number of files successfully finished so far';
COMMENT ON COLUMN jedi_datasets.nfilesonhold IS E'The number of files on hold';
COMMENT ON COLUMN jedi_datasets.nfilestobeused IS E'The number of files to be used';
COMMENT ON COLUMN jedi_datasets.nfilesused IS E'The number of files used so far';
COMMENT ON COLUMN jedi_datasets.nfileswaiting IS E'The number of files waiting for real co-jumbo jobs';
COMMENT ON COLUMN jedi_datasets.provenanceid IS E'The DATASETID of the input dataset to which the output dataset is associated. Set NULL for input datasets';
COMMENT ON COLUMN jedi_datasets.site IS E'The replica at the site is used. Set NULL to use the normal brokerage';
COMMENT ON COLUMN jedi_datasets.state IS E'The dataset state of the dataset when it was checked with DDM';
COMMENT ON COLUMN jedi_datasets.statecheckexpiration IS E'The date when the dataset state check is over. Set NULL if the state check is not required any more';
COMMENT ON COLUMN jedi_datasets.statechecktime IS E'Set when the dataset state is checked with DDM';
COMMENT ON COLUMN jedi_datasets.status IS E'Show how the dataset is currently being used in Panda';
COMMENT ON COLUMN jedi_datasets.storagetoken IS E'The token in the storage element where input files are available or output files are put during the task is running';
COMMENT ON COLUMN jedi_datasets.streamname IS E'The name of stream for input files which is used as a placeholder in jobParamsTemplate. For output, STREAMNAME in JEDI_Output_Template is used';
COMMENT ON COLUMN jedi_datasets.templateid IS E'The DATASETID of the template dataset from which the dataset inherits';
COMMENT ON COLUMN jedi_datasets.type IS E'The type of the dataset';
COMMENT ON COLUMN jedi_datasets.vo IS E'The name of virtual organization which owns the dataset';
ALTER TABLE jedi_datasets OWNER TO panda;
CREATE INDEX jedi_datasets_dsetid_idx ON jedi_datasets (datasetid);
CREATE INDEX jedi_datasets_taskid_type_idx ON jedi_datasets (jeditaskid, type);
CREATE INDEX jedi_dataset_containername_idx ON jedi_datasets (containername);
CREATE INDEX jedi_dataset_dnametypetid_idx ON jedi_datasets (datasetname, type, jeditaskid);
CREATE INDEX jedi_dataset_lockedby_idx ON jedi_datasets (lockedby);
CREATE INDEX jedi_dataset_statecheckexp_idx ON jedi_datasets (statecheckexpiration);
ALTER TABLE jedi_datasets ADD PRIMARY KEY (jeditaskid,datasetid);

CREATE TABLE jedi_dataset_contents (
	jeditaskid bigint NOT NULL,
	datasetid bigint NOT NULL,
	fileid bigint NOT NULL,
	creationdate timestamp NOT NULL,
	lastattempttime timestamp,
	lfn varchar(256) NOT NULL,
	guid varchar(64),
	type varchar(20) NOT NULL,
	status varchar(64) NOT NULL,
	fsize bigint,
	checksum varchar(36),
	scope varchar(30),
	attemptnr smallint,
	maxattempt smallint,
	nevents bigint,
	keeptrack smallint,
	startevent bigint,
	endevent bigint,
	firstevent bigint,
	boundaryid bigint,
	pandaid bigint,
	failedattempt smallint,
	lumiblocknr bigint,
	outpandaid bigint,
	maxfailure smallint,
	ramcount bigint DEFAULT 0,
	is_waiting char(1),
	jobsetid bigint,
	proc_status varchar(64)
) PARTITION BY RANGE (jeditaskid) ;
COMMENT ON COLUMN jedi_dataset_contents.attemptnr IS E'How many times the file has been tried so far';
COMMENT ON COLUMN jedi_dataset_contents.boundaryid IS E'Splitting Input to respect this identifier if not NULL. e.g., used to specify lumi block boundaries';
COMMENT ON COLUMN jedi_dataset_contents.checksum IS E'The checksum of the file';
COMMENT ON COLUMN jedi_dataset_contents.creationdate IS E'Set when the record is inserted at the first time';
COMMENT ON COLUMN jedi_dataset_contents.datasetid IS E'The dataset identifier coming from the dataset table';
COMMENT ON COLUMN jedi_dataset_contents.endevent IS E'The ending event number used in the file';
COMMENT ON COLUMN jedi_dataset_contents.failedattempt IS E'How many times the file failed so far';
COMMENT ON COLUMN jedi_dataset_contents.fileid IS E'Auto-incremented ID of the file generated by the JEDI_DATASET_CONT_FILEID_SEQ';
COMMENT ON COLUMN jedi_dataset_contents.firstevent IS E'The event number which is assigned to the first processed event';
COMMENT ON COLUMN jedi_dataset_contents.fsize IS E'The size of the file';
COMMENT ON COLUMN jedi_dataset_contents.guid IS E'The GUID of the file';
COMMENT ON COLUMN jedi_dataset_contents.is_waiting IS E'Set Y if waiting for real co-jumbo jobs';
COMMENT ON COLUMN jedi_dataset_contents.jeditaskid IS E'The task identifier coming from the task table';
COMMENT ON COLUMN jedi_dataset_contents.jobsetid IS E'JobSetID of the job which uses the file';
COMMENT ON COLUMN jedi_dataset_contents.keeptrack IS E'Set 1 when keeping track of the file usage';
COMMENT ON COLUMN jedi_dataset_contents.lastattempttime IS E'Set when the file is tried';
COMMENT ON COLUMN jedi_dataset_contents.lfn IS E'The logical filename';
COMMENT ON COLUMN jedi_dataset_contents.lumiblocknr IS E'Lumiblock Number in the file';
COMMENT ON COLUMN jedi_dataset_contents.maxattempt IS E'How many times the file can be failed at most';
COMMENT ON COLUMN jedi_dataset_contents.maxfailure IS E'How many times the file can be failed at most';
COMMENT ON COLUMN jedi_dataset_contents.nevents IS E'The number of events in the file';
COMMENT ON COLUMN jedi_dataset_contents.outpandaid IS E'PandaID of the job which produced the file.';
COMMENT ON COLUMN jedi_dataset_contents.pandaid IS E'PandaID of the job which uses the file';
COMMENT ON COLUMN jedi_dataset_contents.proc_status IS E'Processing status of the file';
COMMENT ON COLUMN jedi_dataset_contents.ramcount IS E'Increase the RAM requirements at job level (before it was at task level)';
COMMENT ON COLUMN jedi_dataset_contents.scope IS E'The scope of the file';
COMMENT ON COLUMN jedi_dataset_contents.startevent IS E'The starting event number used in the file';
COMMENT ON COLUMN jedi_dataset_contents.status IS E'The status of the file';
COMMENT ON COLUMN jedi_dataset_contents.type IS E'The type of the file';
ALTER TABLE jedi_dataset_contents OWNER TO panda;
CREATE INDEX jedi_datasetcontent_lfn_idx ON jedi_dataset_contents (lfn);
CREATE INDEX jedi_dataset_contents_id_idx ON jedi_dataset_contents (datasetid);
CREATE INDEX jedi_dataset_contents_pid_idx ON jedi_dataset_contents (pandaid);
ALTER TABLE jedi_dataset_contents ADD PRIMARY KEY (jeditaskid,datasetid,fileid);

CREATE TABLE jedi_dataset_locality (
	jeditaskid bigint NOT NULL,
	datasetid bigint NOT NULL,
	rse varchar(64) NOT NULL,
	timestamp timestamp NOT NULL
) ;
ALTER TABLE jedi_dataset_locality OWNER TO panda;
ALTER TABLE jedi_dataset_locality ADD PRIMARY KEY (jeditaskid,datasetid,rse);

CREATE TABLE jedi_events (
	jeditaskid bigint NOT NULL,
	pandaid bigint NOT NULL,
	fileid bigint NOT NULL,
	job_processid bigint NOT NULL,
	datasetid bigint NOT NULL,
	status smallint NOT NULL,
	def_min_eventid bigint,
	def_max_eventid bigint,
	processed_upto_eventid bigint,
	attemptnr smallint,
	objstore_id bigint,
	event_offset bigint,
	is_jumbo smallint,
	ziprow_id bigint,
	file_not_deleted char(1),
	error_code integer,
	path_convention smallint
) PARTITION BY RANGE (jeditaskid) ;
COMMENT ON COLUMN jedi_events.attemptnr IS E'How many times the events have been retried so far';
COMMENT ON COLUMN jedi_events.datasetid IS E'DatasetID of the file which contains the events';
COMMENT ON COLUMN jedi_events.def_max_eventid IS E'The maximum event number which is assigned to the slave process';
COMMENT ON COLUMN jedi_events.def_min_eventid IS E'The minimum event number which is assigned to the slave process';
COMMENT ON COLUMN jedi_events.error_code IS E'Error code for the event range';
COMMENT ON COLUMN jedi_events.event_offset IS E'The total offset of positional event numbers';
COMMENT ON COLUMN jedi_events.file_not_deleted IS E'Flag whether the file has been deleted by Rucio. The flag is set to ''Y'' by the PanDA server for "status IN (4,5,7,8) AND objstore_id IS NOT NULL" condition. Rucio sets it back to NULL after handling it.';
COMMENT ON COLUMN jedi_events.fileid IS E'FileID of the file which contains the events';
COMMENT ON COLUMN jedi_events.is_jumbo IS E'Set 1 if events are processed by a jumbo job';
COMMENT ON COLUMN jedi_events.job_processid IS E'identifier of the slave process';
COMMENT ON COLUMN jedi_events.objstore_id IS E'Identifier of the objectstore endpoint where pre-merged file is stored';
COMMENT ON COLUMN jedi_events.pandaid IS E'PandaID of the job in which the events are processed';
COMMENT ON COLUMN jedi_events.path_convention IS E'Convention ID of file path. This is required to use various conventions for file paths in object stores.';
COMMENT ON COLUMN jedi_events.processed_upto_eventid IS E'The event number which the slave process completed so far';
COMMENT ON COLUMN jedi_events.status IS E'The status of the event range';
COMMENT ON COLUMN jedi_events.ziprow_id IS E'ROW_ID of the zip file in the file table (FILESTABLE4). NULL if not zipped';
ALTER TABLE jedi_events OWNER TO panda;
CREATE INDEX jedi_events_fileid_idx ON jedi_events (fileid);
CREATE INDEX jedi_events_file_notdel_idx ON jedi_events (file_not_deleted);
CREATE INDEX jedi_events_pandaid_status_idx ON jedi_events (pandaid, status);
ALTER TABLE jedi_events ADD PRIMARY KEY (jeditaskid,pandaid,fileid,job_processid);

CREATE TABLE jedi_jobparams_template (
	jeditaskid bigint NOT NULL,
	jobparamstemplate text
) PARTITION BY RANGE (jeditaskid) ;
COMMENT ON COLUMN jedi_jobparams_template.jeditaskid IS E'The task identifier coming from the task table';
COMMENT ON COLUMN jedi_jobparams_template.jobparamstemplate IS E'The template to generate job parameters';
ALTER TABLE jedi_jobparams_template OWNER TO panda;
ALTER TABLE jedi_jobparams_template ADD PRIMARY KEY (jeditaskid);

CREATE TABLE jedi_job_retry_history (
	jeditaskid bigint NOT NULL,
	oldpandaid bigint NOT NULL,
	newpandaid bigint NOT NULL,
	ins_utc_tstamp timestamp DEFAULT ((CURRENT_TIMESTAMP(0) AT TIME ZONE 'UTC')),
	relationtype varchar(16),
	originpandaid bigint
) PARTITION BY RANGE (ins_utc_tstamp) ;
COMMENT ON COLUMN jedi_job_retry_history.jeditaskid IS E'JediTaskID of the jobs';
COMMENT ON COLUMN jedi_job_retry_history.newpandaid IS E'PandaID of the new job';
COMMENT ON COLUMN jedi_job_retry_history.oldpandaid IS E'PandaID of the old job';
ALTER TABLE jedi_job_retry_history OWNER TO panda;
CREATE UNIQUE INDEX jedi_job_retry_history_uq ON jedi_job_retry_history (jeditaskid, newpandaid, oldpandaid, originpandaid, ins_utc_tstamp);
CREATE INDEX jedi_job_retry_hist_oldpid_idx ON jedi_job_retry_history (oldpandaid);
CREATE INDEX jedi_job_retry_hist_origid_idx ON jedi_job_retry_history (originpandaid);
ALTER TABLE jedi_job_retry_history ADD UNIQUE (jeditaskid,oldpandaid,newpandaid,originpandaid,ins_utc_tstamp);

CREATE TABLE jedi_output_template (
	jeditaskid bigint NOT NULL,
	datasetid bigint NOT NULL,
	outtempid bigint NOT NULL,
	filenametemplate varchar(256) NOT NULL,
	maxserialnr integer,
	serialnr integer,
	sourcename varchar(256),
	streamname varchar(20),
	outtype varchar(20)
) PARTITION BY RANGE (jeditaskid) ;
COMMENT ON COLUMN jedi_output_template.datasetid IS E'The dataset identifier coming from the dataset table';
COMMENT ON COLUMN jedi_output_template.filenametemplate IS E'The template to generate output filenames';
COMMENT ON COLUMN jedi_output_template.jeditaskid IS E'The task identifier coming from the task table';
COMMENT ON COLUMN jedi_output_template.maxserialnr IS E'The maximum serial number which can be used to generate files using the template';
COMMENT ON COLUMN jedi_output_template.outtempid IS E'Auto-incremented ID of the template generated by the JEDI_OUTPUT_TEMPLATE_ID_SEQ';
COMMENT ON COLUMN jedi_output_template.outtype IS E'The type of the file';
COMMENT ON COLUMN jedi_output_template.serialnr IS E'The serial number to generate files using the template. When a new file is produced it would have SERIALNR+1 and this field is incremented';
COMMENT ON COLUMN jedi_output_template.sourcename IS E'source filename to be renamed, if any. Set NULL if renaming is unnecessary';
COMMENT ON COLUMN jedi_output_template.streamname IS E'The name of the stream which is used as a placeholder in jobParamsTemplate';
ALTER TABLE jedi_output_template OWNER TO panda;
ALTER TABLE jedi_output_template ADD PRIMARY KEY (jeditaskid,datasetid,outtempid);

CREATE TABLE jedi_process_lock (
	vo varchar(16) NOT NULL,
	prodsourcelabel varchar(20) NOT NULL,
	workqueue_id integer NOT NULL,
	cloud varchar(10) NOT NULL,
	lockedby varchar(40),
	lockedtime timestamp,
	resource_type varchar(56) NOT NULL,
	component varchar(56) NOT NULL
) ;
COMMENT ON TABLE jedi_process_lock IS E'Table for synchronisation of JEDI processes';
COMMENT ON COLUMN jedi_process_lock.cloud IS E'The cloud name';
COMMENT ON COLUMN jedi_process_lock.component IS E'PanDA component (job generator, watchdog...) that wants to set the lock';
COMMENT ON COLUMN jedi_process_lock.lockedby IS E'The name of process/thread which uses the lock';
COMMENT ON COLUMN jedi_process_lock.lockedtime IS E'Set when the lock is created';
COMMENT ON COLUMN jedi_process_lock.prodsourcelabel IS E'The source label, such as managed, user, prod_test, ...';
COMMENT ON COLUMN jedi_process_lock.resource_type IS E'Resource type (SCORE, MCORE...) for the lock';
COMMENT ON COLUMN jedi_process_lock.vo IS E'The name of virtual organization';
COMMENT ON COLUMN jedi_process_lock.workqueue_id IS E'The work queue identifier';
ALTER TABLE jedi_process_lock OWNER TO panda;
ALTER TABLE jedi_process_lock ADD PRIMARY KEY (vo,prodsourcelabel,workqueue_id,cloud,resource_type,component);

CREATE TABLE jedi_taskparams (
	jeditaskid bigint NOT NULL,
	taskparams text
) PARTITION BY RANGE (jeditaskid) ;
COMMENT ON COLUMN jedi_taskparams.jeditaskid IS E'The task identifier coming from the task table';
COMMENT ON COLUMN jedi_taskparams.taskparams IS E'Special task parameters. Eg, the list of lost files for a lost-file recovery task';
ALTER TABLE jedi_taskparams OWNER TO panda;
ALTER TABLE jedi_taskparams ADD PRIMARY KEY (jeditaskid);

CREATE TABLE jedi_tasks (
	jeditaskid bigint NOT NULL,
	taskname varchar(256),
	status varchar(64) NOT NULL,
	username varchar(128) NOT NULL,
	creationdate timestamp NOT NULL,
	modificationtime timestamp NOT NULL,
	reqid integer,
	oldstatus varchar(64),
	cloud varchar(10),
	site varchar(60),
	starttime timestamp,
	endtime timestamp,
	frozentime timestamp,
	prodsourcelabel varchar(20),
	workinggroup varchar(32),
	vo varchar(16),
	corecount integer,
	tasktype varchar(64),
	processingtype varchar(64),
	taskpriority integer,
	currentpriority integer,
	architecture varchar(256),
	transuses varchar(64),
	transhome varchar(128),
	transpath varchar(128),
	lockedby varchar(40),
	lockedtime timestamp,
	termcondition varchar(100),
	splitrule varchar(200),
	walltime integer,
	walltimeunit varchar(32),
	outdiskcount integer,
	outdiskunit varchar(32),
	workdiskcount integer,
	workdiskunit varchar(32),
	ramcount integer,
	ramunit varchar(32),
	iointensity integer,
	iointensityunit varchar(32),
	workqueue_id integer,
	progress smallint,
	failurerate smallint,
	errordialog varchar(255),
	countrygroup varchar(20),
	parent_tid bigint,
	eventservice smallint,
	ticketid varchar(50),
	ticketsystemtype varchar(16),
	statechangetime timestamp,
	superstatus varchar(64),
	campaign varchar(32),
	mergeramcount integer,
	mergeramunit varchar(32),
	mergewalltime integer,
	mergewalltimeunit varchar(32),
	throttledtime timestamp,
	numthrottled smallint,
	mergecorecount integer,
	goal smallint,
	assessmenttime timestamp,
	cputime integer,
	cputimeunit varchar(32),
	cpuefficiency smallint,
	basewalltime integer,
	amiflag_old varchar(10),
	amiflag integer DEFAULT 1,
	nucleus varchar(52),
	baseramcount integer,
	ttcrequested timestamp,
	ttcpredicted timestamp,
	ttcpredictiondate timestamp,
	rescuetime timestamp,
	requesttype varchar(32),
	gshare varchar(32),
	usejumbo char(1),
	resource_type varchar(56),
	diskio integer,
	diskiounit varchar(32),
	memory_leak_core bigint,
	memory_leak_x2 bigint,
	attemptnr smallint,
	container_name varchar(200),
	job_label varchar(20)
) PARTITION BY RANGE (jeditaskid) ;
COMMENT ON COLUMN jedi_tasks.amiflag IS E'It will contain a mask, one bit per AMI task (AMI has two tasks) with default value at insertion for "amiflag" to 3 (0b00000011). A trigger when the field âcampaignâ is modified:	if "amiflag" is NULL then "amiflag" = 2 else "amiflag" = BITOR(AMIFLAG, 2)';
COMMENT ON COLUMN jedi_tasks.architecture IS E'The architecture on which the task runs. Eg, $CMTCONFIG';
COMMENT ON COLUMN jedi_tasks.assessmenttime IS E'Set when statistics is updated';
COMMENT ON COLUMN jedi_tasks.baseramcount IS E'Shared memory size in MB. The brokerage requires it to estimate memory usage based on quantities measured by the MemoryMonitor.';
COMMENT ON COLUMN jedi_tasks.basewalltime IS E'Offset of walltime in sec';
COMMENT ON COLUMN jedi_tasks.campaign IS E'Campaign to which the task belongs';
COMMENT ON COLUMN jedi_tasks.cloud IS E'The cloud where the task is assigned';
COMMENT ON COLUMN jedi_tasks.corecount IS E'The number of cores per job. Set 0 if it can vary';
COMMENT ON COLUMN jedi_tasks.countrygroup IS E'The name of the country group to which the user belongs';
COMMENT ON COLUMN jedi_tasks.cpuefficiency IS E'Percentage of single core efficiency';
COMMENT ON COLUMN jedi_tasks.cputime IS E'The product of HS06 score and CPU consumption time per event';
COMMENT ON COLUMN jedi_tasks.cputimeunit IS E'Unit of CPUTIMECOUNT (HS06sPerEvent)';
COMMENT ON COLUMN jedi_tasks.creationdate IS E'Set when the record is inserted at the first time';
COMMENT ON COLUMN jedi_tasks.currentpriority IS E'The current priority of the task. JEDI could increase/decrease priorities if necessary';
COMMENT ON COLUMN jedi_tasks.diskio IS E'Local disk access measured by scouts. (totWBytes+totRBytes)/(endTime-startTime)';
COMMENT ON COLUMN jedi_tasks.diskiounit IS E'Unit of DISKIO';
COMMENT ON COLUMN jedi_tasks.endtime IS E'Set when the task is finished';
COMMENT ON COLUMN jedi_tasks.errordialog IS E'Error diagnostics';
COMMENT ON COLUMN jedi_tasks.eventservice IS E'The task uses Event Service if 1';
COMMENT ON COLUMN jedi_tasks.failurerate IS E'The frequency of job failures in the task (in %)';
COMMENT ON COLUMN jedi_tasks.frozentime IS E'Set when the record is frozen';
COMMENT ON COLUMN jedi_tasks.goal IS E'Goal of the task completion in percentage';
COMMENT ON COLUMN jedi_tasks.gshare IS E'Global share';
COMMENT ON COLUMN jedi_tasks.iointensity IS E'The characteristics of I/O patterns measured by scouts';
COMMENT ON COLUMN jedi_tasks.iointensityunit IS E'unit of IOINTENSITY';
COMMENT ON COLUMN jedi_tasks.jeditaskid IS E'The task identifier coming from ATLAS_DEFT.PRODSYS2_TASK_ID_SEQ';
COMMENT ON COLUMN jedi_tasks.lockedby IS E'The name of process/thread which is taking care of the record';
COMMENT ON COLUMN jedi_tasks.lockedtime IS E'Set when the record is locked';
COMMENT ON COLUMN jedi_tasks.memory_leak_core IS E'Average memory leak (kB/s) per core for the task, as measured by the task';
COMMENT ON COLUMN jedi_tasks.memory_leak_x2 IS E'Memory leak chi square statistic';
COMMENT ON COLUMN jedi_tasks.mergecorecount IS E'The number of cores per merge job. Set 0 if it can vary.';
COMMENT ON COLUMN jedi_tasks.mergeramcount IS E'average memory consumption for merge jobs measured by scouts';
COMMENT ON COLUMN jedi_tasks.mergeramunit IS E'unit of MERGERAMCOUNT';
COMMENT ON COLUMN jedi_tasks.mergewalltime IS E'average walltime consumption for merge jobs measured by scouts when processing 1MB of input';
COMMENT ON COLUMN jedi_tasks.mergewalltimeunit IS E'unit of MERGEWALLTIME';
COMMENT ON COLUMN jedi_tasks.modificationtime IS E'Set when the record is updated';
COMMENT ON COLUMN jedi_tasks.nucleus IS E'Name of the site where the task is assigned in WORLD cloud';
COMMENT ON COLUMN jedi_tasks.numthrottled IS E'How many times the task was throttled so far';
COMMENT ON COLUMN jedi_tasks.oldstatus IS E'Old task status';
COMMENT ON COLUMN jedi_tasks.outdiskcount IS E'average total size of outputs measured by scouts when processing 1MB of input';
COMMENT ON COLUMN jedi_tasks.outdiskunit IS E'unit of OUTDISKCOUNT';
COMMENT ON COLUMN jedi_tasks.parent_tid IS E'The jediTaskID of the parent task. PARENT_TID=JEDITASKID if no parent';
COMMENT ON COLUMN jedi_tasks.processingtype IS E'The type of the process';
COMMENT ON COLUMN jedi_tasks.prodsourcelabel IS E'The source label, such as managed, user, prod_test, ...';
COMMENT ON COLUMN jedi_tasks.progress IS E'Percentage of completion of the task';
COMMENT ON COLUMN jedi_tasks.ramcount IS E'average memory consumption measured by scouts';
COMMENT ON COLUMN jedi_tasks.ramunit IS E'unit of RAMCOUNT';
COMMENT ON COLUMN jedi_tasks.requesttype IS E'Type of the request';
COMMENT ON COLUMN jedi_tasks.rescuetime IS E'Update when rescue process checks the task';
COMMENT ON COLUMN jedi_tasks.resource_type IS E'Resource type name';
COMMENT ON COLUMN jedi_tasks.site IS E'The site where the task is assigned. Set NULL to use the normal brokerage';
COMMENT ON COLUMN jedi_tasks.splitrule IS E'Rules for job splitting. Eg, to use event-level splitting or not, to limit the number of files per job, etc';
COMMENT ON COLUMN jedi_tasks.starttime IS E'Set when the task gets started';
COMMENT ON COLUMN jedi_tasks.statechangetime IS E'Updated when the task status is changed';
COMMENT ON COLUMN jedi_tasks.status IS E'The task status';
COMMENT ON COLUMN jedi_tasks.superstatus IS E'The super status (DEFT status) of the task which is explained in DEFT task status';
COMMENT ON COLUMN jedi_tasks.taskname IS E'The task name';
COMMENT ON COLUMN jedi_tasks.taskpriority IS E'The assigned priority of the task';
COMMENT ON COLUMN jedi_tasks.tasktype IS E'The task type';
COMMENT ON COLUMN jedi_tasks.termcondition IS E'Termination condition of the task. Eg, to terminate task when 90% of files are successfully processed';
COMMENT ON COLUMN jedi_tasks.throttledtime IS E'Updated when the task is throttled';
COMMENT ON COLUMN jedi_tasks.ticketid IS E'The identifier of the ticket to keep track of the task';
COMMENT ON COLUMN jedi_tasks.ticketsystemtype IS E'The type of the ticket system';
COMMENT ON COLUMN jedi_tasks.transhome IS E'The name of the software cache in which the transformation is included';
COMMENT ON COLUMN jedi_tasks.transpath IS E'The name of the transformation';
COMMENT ON COLUMN jedi_tasks.transuses IS E'The name of the software release';
COMMENT ON COLUMN jedi_tasks.ttcpredicted IS E'Time To Completion predicted by the system';
COMMENT ON COLUMN jedi_tasks.ttcpredictiondate IS E'Timestamp when the TTCPREDICTED was calculated';
COMMENT ON COLUMN jedi_tasks.ttcrequested IS E'Time To Completion expected by the task submitter';
COMMENT ON COLUMN jedi_tasks.usejumbo IS E'Set Y when using jumbo jobs';
COMMENT ON COLUMN jedi_tasks.username IS E'The name of the user who owns the task';
COMMENT ON COLUMN jedi_tasks.vo IS E'The name of virtual organization which owns the task ';
COMMENT ON COLUMN jedi_tasks.walltime IS E'average walltime consumption measured by scouts when processing 1MB of input';
COMMENT ON COLUMN jedi_tasks.walltimeunit IS E'unit of WALLTIME';
COMMENT ON COLUMN jedi_tasks.workdiskcount IS E'average size of work directory measured by scouts';
COMMENT ON COLUMN jedi_tasks.workdiskunit IS E'unit of WORKDISKCOUNT';
COMMENT ON COLUMN jedi_tasks.workinggroup IS E'The name of the working group which owns the task ';
COMMENT ON COLUMN jedi_tasks.workqueue_id IS E'The work queue identifier to which the task belongs';
ALTER TABLE jedi_tasks OWNER TO panda;
CREATE INDEX jedi_tasks_amiflag_idx ON jedi_tasks (amiflag);
CREATE INDEX jedi_tasks_creation_idx ON jedi_tasks (creationdate);
CREATE INDEX jedi_tasks_lockedby_idx ON jedi_tasks (lockedby);
CREATE INDEX jedi_tasks_modiftime_idx ON jedi_tasks (modificationtime);
CREATE INDEX jedi_tasks_nametaskid_idx ON jedi_tasks (taskname, jeditaskid);
CREATE INDEX jedi_tasks_parent_tid_idx ON jedi_tasks (parent_tid);
CREATE INDEX jedi_tasks_status3attr_idx ON jedi_tasks (status, workqueue_id, prodsourcelabel, jeditaskid);
CREATE INDEX jedi_upper_tasks_name_idx ON jedi_tasks (upper(taskname));
ALTER TABLE jedi_tasks ADD PRIMARY KEY (jeditaskid);

CREATE TABLE jedi_tasks_13nov2015 (
	jeditaskid bigint NOT NULL,
	taskname varchar(132),
	status varchar(64) NOT NULL,
	username varchar(128) NOT NULL,
	creationdate timestamp NOT NULL,
	modificationtime timestamp NOT NULL,
	reqid integer,
	oldstatus varchar(64),
	cloud varchar(10),
	site varchar(60),
	starttime timestamp,
	endtime timestamp,
	frozentime timestamp,
	prodsourcelabel varchar(20),
	workinggroup varchar(32),
	vo varchar(16),
	corecount integer,
	tasktype varchar(64),
	processingtype varchar(64),
	taskpriority integer,
	currentpriority integer,
	architecture varchar(256),
	transuses varchar(64),
	transhome varchar(128),
	transpath varchar(128),
	lockedby varchar(40),
	lockedtime timestamp,
	termcondition varchar(100),
	splitrule varchar(100),
	walltime integer,
	walltimeunit varchar(32),
	outdiskcount integer,
	outdiskunit varchar(32),
	workdiskcount integer,
	workdiskunit varchar(32),
	ramcount integer,
	ramunit varchar(32),
	iointensity integer,
	iointensityunit varchar(32),
	workqueue_id integer,
	progress smallint,
	failurerate smallint,
	errordialog varchar(255),
	countrygroup varchar(20),
	parent_tid bigint,
	eventservice smallint,
	ticketid varchar(50),
	ticketsystemtype varchar(16),
	statechangetime timestamp,
	superstatus varchar(64),
	campaign varchar(32),
	mergeramcount integer,
	mergeramunit varchar(32),
	mergewalltime integer,
	mergewalltimeunit varchar(32),
	throttledtime timestamp,
	numthrottled smallint,
	mergecorecount integer,
	goal smallint,
	assessmenttime timestamp,
	cputime integer,
	cputimeunit varchar(32),
	cpuefficiency smallint,
	basewalltime integer,
	amiflag_old varchar(10),
	amiflag integer,
	nucleus varchar(52),
	baseramcount integer
) ;
ALTER TABLE jedi_tasks_13nov2015 OWNER TO panda;

CREATE TABLE jedi_work_queue (
	queue_id integer NOT NULL,
	queue_name varchar(16) NOT NULL,
	queue_type varchar(16) NOT NULL,
	vo varchar(16) NOT NULL,
	status varchar(64),
	partitionid integer,
	stretchable smallint,
	queue_share smallint,
	queue_order smallint,
	criteria varchar(256),
	variables varchar(256),
	queue_function varchar(32)
) ;
COMMENT ON COLUMN jedi_work_queue.criteria IS E'Selection criteria written in SQL to map tasks/jobs to the queue. It is applied to JEDI_Tasks';
COMMENT ON COLUMN jedi_work_queue.partitionid IS E'The QUEUE_ID of the partition to which the queue belong. Set NULL if the queue doesn''t belong to any partition. Set -1 if the record stands for a partition';
COMMENT ON COLUMN jedi_work_queue.queue_function IS E'Function of workqueue. Special queues should be tagged as Resource';
COMMENT ON COLUMN jedi_work_queue.queue_id IS E'unique identifier';
COMMENT ON COLUMN jedi_work_queue.queue_name IS E'The name of the queue';
COMMENT ON COLUMN jedi_work_queue.queue_order IS E'The queue is evaluated before other queues which have higher ORDER values. If a task or a job passes the selection criteria, it is mapped to the queue and no more queues are evaluated';
COMMENT ON COLUMN jedi_work_queue.queue_share IS E'Share for the queue. Set NULL if the queue is not throttled';
COMMENT ON COLUMN jedi_work_queue.queue_type IS E'The type of the queue which is mapped to prodSourceLabel in JEDI_Tasks and jobs*Table';
COMMENT ON COLUMN jedi_work_queue.status IS E'The queue status';
COMMENT ON COLUMN jedi_work_queue.stretchable IS E'Set 1 if the queue can use shares of other queues when those queues don''t have tasks/jobs';
COMMENT ON COLUMN jedi_work_queue.variables IS E'Valid values to replace bind-variables in CRITERIA';
COMMENT ON COLUMN jedi_work_queue.vo IS E'The name of virtual organization for which the queue is used';
ALTER TABLE jedi_work_queue OWNER TO panda;
CREATE INDEX jedi_work_queue_qfunc_idx ON jedi_work_queue (queue_function, queue_id);
ALTER TABLE jedi_work_queue ADD UNIQUE (queue_name,queue_type,vo);
ALTER TABLE jedi_work_queue ADD PRIMARY KEY (queue_id);

CREATE TABLE jobparamstable (
	pandaid bigint NOT NULL,
	modificationtime timestamp NOT NULL DEFAULT to_date('01-JAN-1970 00:00:00','DD-MON-YYYY HH24:MI:SS'),
	jobparameters text
) PARTITION BY RANGE (modificationtime) ;
COMMENT ON TABLE jobparamstable IS E'Table with information on the job input parameters. When a PanDA job is in a defined or running state, relevant rows reside in the INITIAL partition of the table. When the job is finished or aborted the "modificationtime" is set the to real current time and as the table has "row movement" enabled, Oracle moves the rows from the INITIAL partition to the partitions of the current day. Data is regularly copied to an archive table in ATLAS_PANDAARCH schema. Data retention is defined to be 3 days (can be changed if necessary)';
COMMENT ON COLUMN jobparamstable.jobparameters IS E'the parameters of the job. The column type is CLOB';
COMMENT ON COLUMN jobparamstable.modificationtime IS E'modificationTime of the job (in UTC)';
COMMENT ON COLUMN jobparamstable.pandaid IS E'PandaID of the job';
ALTER TABLE jobparamstable OWNER TO panda;
ALTER TABLE jobparamstable ADD PRIMARY KEY (pandaid,modificationtime);

CREATE TABLE jobsactive4 (
	pandaid bigint NOT NULL DEFAULT '0',
	jobdefinitionid bigint NOT NULL DEFAULT '0',
	schedulerid varchar(128),
	pilotid varchar(200),
	creationtime timestamp NOT NULL DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	creationhost varchar(128),
	modificationtime timestamp NOT NULL DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	modificationhost varchar(128),
	atlasrelease varchar(64),
	transformation varchar(250),
	homepackage varchar(80),
	prodserieslabel varchar(20) DEFAULT 'Rome',
	prodsourcelabel varchar(20) DEFAULT 'managed',
	produserid varchar(250),
	assignedpriority integer NOT NULL DEFAULT '0',
	currentpriority integer NOT NULL DEFAULT '0',
	attemptnr smallint NOT NULL DEFAULT '0',
	maxattempt smallint NOT NULL DEFAULT '0',
	jobstatus varchar(15) NOT NULL DEFAULT 'activated',
	jobname varchar(256),
	maxcpucount bigint NOT NULL DEFAULT '0',
	maxcpuunit varchar(32),
	maxdiskcount bigint NOT NULL DEFAULT '0',
	maxdiskunit char(4),
	ipconnectivity char(5),
	minramcount bigint NOT NULL DEFAULT '0',
	minramunit char(2),
	starttime timestamp DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	endtime timestamp DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	cpuconsumptiontime numeric(20) NOT NULL DEFAULT '0',
	cpuconsumptionunit varchar(128),
	commandtopilot varchar(250),
	transexitcode varchar(128),
	piloterrorcode integer NOT NULL DEFAULT '0',
	piloterrordiag varchar(500),
	exeerrorcode integer NOT NULL DEFAULT '0',
	exeerrordiag varchar(500),
	superrorcode integer NOT NULL DEFAULT '0',
	superrordiag varchar(250),
	ddmerrorcode integer NOT NULL DEFAULT '0',
	ddmerrordiag varchar(500),
	brokerageerrorcode integer NOT NULL DEFAULT '0',
	brokerageerrordiag varchar(250),
	jobdispatchererrorcode integer NOT NULL DEFAULT '0',
	jobdispatchererrordiag varchar(250),
	taskbuffererrorcode integer NOT NULL DEFAULT '0',
	taskbuffererrordiag varchar(300),
	computingsite varchar(128),
	computingelement varchar(128),
	jobparameters text,
	metadata text,
	proddblock varchar(255),
	dispatchdblock varchar(255),
	destinationdblock varchar(255),
	destinationse varchar(250),
	nevents bigint NOT NULL DEFAULT '0',
	grid varchar(50),
	cloud varchar(50),
	cpuconversion decimal(9,4),
	sourcesite varchar(36),
	destinationsite varchar(36),
	transfertype varchar(10),
	taskid integer,
	cmtconfig varchar(250),
	statechangetime timestamp DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	proddbupdatetime timestamp DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	lockedby varchar(128),
	relocationflag smallint DEFAULT '0',
	jobexecutionid bigint DEFAULT '0',
	vo varchar(16),
	pilottiming varchar(100),
	workinggroup varchar(20),
	processingtype varchar(64),
	produsername varchar(60),
	ninputfiles integer,
	countrygroup varchar(20),
	batchid varchar(80),
	parentid bigint,
	specialhandling varchar(80),
	jobsetid bigint,
	corecount smallint,
	ninputdatafiles integer,
	inputfiletype varchar(32),
	inputfileproject varchar(64),
	inputfilebytes bigint,
	noutputdatafiles integer,
	outputfilebytes bigint,
	jobmetrics varchar(500),
	workqueue_id integer,
	jeditaskid bigint,
	jobsubstatus varchar(80),
	actualcorecount integer,
	reqid integer,
	maxrss bigint,
	maxvmem bigint,
	maxswap bigint,
	maxpss bigint,
	avgrss bigint,
	avgvmem bigint,
	avgswap bigint,
	avgpss bigint,
	maxwalltime bigint,
	nucleus varchar(52),
	eventservice smallint,
	failedattempt smallint,
	hs06sec bigint,
	gshare varchar(32),
	hs06 bigint,
	totrchar bigint,
	totwchar bigint,
	totrbytes bigint,
	totwbytes bigint,
	raterchar bigint,
	ratewchar bigint,
	raterbytes bigint,
	ratewbytes bigint,
	resource_type varchar(56),
	diskio integer,
	memory_leak bigint,
	memory_leak_x2 bigint,
	container_name varchar(200),
	job_label varchar(20),
	meancorecount decimal(8,2)
) ;
COMMENT ON TABLE jobsactive4 IS E'Table for hosting all PanDA jobs that are in active running mode. All timestamp and date type columns are in UTC';
COMMENT ON COLUMN jobsactive4.assignedpriority IS E'defined priority value';
COMMENT ON COLUMN jobsactive4.atlasrelease IS E'Release required to run the job';
COMMENT ON COLUMN jobsactive4.attemptnr IS E'how many times the job was retried so far';
COMMENT ON COLUMN jobsactive4.avgpss IS E'Average PSS in the job';
COMMENT ON COLUMN jobsactive4.avgrss IS E'AverageÂ RSS in the job';
COMMENT ON COLUMN jobsactive4.avgswap IS E'Average SWAP in the job';
COMMENT ON COLUMN jobsactive4.avgvmem IS E'Average VMEM in the job';
COMMENT ON COLUMN jobsactive4.batchid IS E'ID of the job in the backend batch system';
COMMENT ON COLUMN jobsactive4.brokerageerrorcode IS E'brokerage error code';
COMMENT ON COLUMN jobsactive4.brokerageerrordiag IS E'brokerage error diagnostics';
COMMENT ON COLUMN jobsactive4.cloud IS E'cloud (associated with Tier 1) where the job is submitted to';
COMMENT ON COLUMN jobsactive4.cmtconfig IS E'cmt config';
COMMENT ON COLUMN jobsactive4.commandtopilot IS E'used to send commands to pilot from Panda server, to kill the job for instance';
COMMENT ON COLUMN jobsactive4.computingelement IS E'name of computing element';
COMMENT ON COLUMN jobsactive4.computingsite IS E'site name where the job runs';
COMMENT ON COLUMN jobsactive4.corecount IS E'the number of CPU cores';
COMMENT ON COLUMN jobsactive4.countrygroup IS E'country of the job submitter';
COMMENT ON COLUMN jobsactive4.cpuconsumptiontime IS E'actual execution time of the job';
COMMENT ON COLUMN jobsactive4.cpuconsumptionunit IS E'unit for CPUCONSUMPTIONTIME';
COMMENT ON COLUMN jobsactive4.cpuconversion IS E'CPU conversion factor';
COMMENT ON COLUMN jobsactive4.creationhost IS E'the hostname where the job is submitted';
COMMENT ON COLUMN jobsactive4.creationtime IS E'generated by Oracle''s SYSDATE function when the job is inserted to jobsDefined4';
COMMENT ON COLUMN jobsactive4.currentpriority IS E'actual priority value which is usually the same as assignedPriority, can be modified by Panda server';
COMMENT ON COLUMN jobsactive4.ddmerrorcode IS E'DDM error code';
COMMENT ON COLUMN jobsactive4.ddmerrordiag IS E'DDM error diagnostics';
COMMENT ON COLUMN jobsactive4.destinationdblock IS E'name of destination dataset for the job; is used to register the outputs of an associated set of jobs as belonging to one block to be saved at an archival destination';
COMMENT ON COLUMN jobsactive4.destinationse IS E'destination storage element of job output files';
COMMENT ON COLUMN jobsactive4.destinationsite IS E'destination site (usually SE) for file transfer';
COMMENT ON COLUMN jobsactive4.diskio IS E'Local disk access in kBPerSec. Required to limit the number of running jobs based on total IO for each queue.';
COMMENT ON COLUMN jobsactive4.dispatchdblock IS E'name of dispatch dataset for the job ; a prodDBlock may be broken down into smaller blocks for dispatch to sites';
COMMENT ON COLUMN jobsactive4.endtime IS E'end time when the job finished on WN';
COMMENT ON COLUMN jobsactive4.eventservice IS E'The job uses Event Service';
COMMENT ON COLUMN jobsactive4.exeerrorcode IS E'executor error code';
COMMENT ON COLUMN jobsactive4.exeerrordiag IS E'executor error diagnostics';
COMMENT ON COLUMN jobsactive4.failedattempt IS E'How many times the input files were failed so far. The maximum number is used if there are multiple input files';
COMMENT ON COLUMN jobsactive4.grid IS E'GRID where the job is submitted to';
COMMENT ON COLUMN jobsactive4.gshare IS E'Global share';
COMMENT ON COLUMN jobsactive4.homepackage IS E'Cache required to run the job';
COMMENT ON COLUMN jobsactive4.hs06 IS E'Core count x core power';
COMMENT ON COLUMN jobsactive4.hs06sec IS E'The product of HS06 score and CPU consumption time';
COMMENT ON COLUMN jobsactive4.inputfilebytes IS E'the total size of input files';
COMMENT ON COLUMN jobsactive4.inputfileproject IS E'project name of input files';
COMMENT ON COLUMN jobsactive4.inputfiletype IS E'type of input files';
COMMENT ON COLUMN jobsactive4.ipconnectivity IS E'defined in prodDB (unused in Panda)';
COMMENT ON COLUMN jobsactive4.jobdefinitionid IS E'comes from ejobdef.jobdefid for managed production jobs, and defines job set IDs for analysis jobs';
COMMENT ON COLUMN jobsactive4.jobdispatchererrorcode IS E'jobDispatcher error code';
COMMENT ON COLUMN jobsactive4.jobdispatchererrordiag IS E'jobDispatcher error diagnostics';
COMMENT ON COLUMN jobsactive4.jobexecutionid IS E'job execution ID coming from ProdDB';
COMMENT ON COLUMN jobsactive4.jobmetrics IS E'a general-purpose field to record various job metrics';
COMMENT ON COLUMN jobsactive4.jobname IS E'the job name defined in prodDB';
COMMENT ON COLUMN jobsactive4.jobparameters IS E'unused. moved to ATLAS_PANDA.JOBPARAMSTABLE. remains for code compatibility';
COMMENT ON COLUMN jobsactive4.jobsetid IS E'jobset ID for analysis jobs';
COMMENT ON COLUMN jobsactive4.jobstatus IS E'status of the job';
COMMENT ON COLUMN jobsactive4.lockedby IS E'the name of the current writer of the record in ProdDB';
COMMENT ON COLUMN jobsactive4.maxattempt IS E'how many times the job can be retried';
COMMENT ON COLUMN jobsactive4.maxcpucount IS E'expected execution time of the job';
COMMENT ON COLUMN jobsactive4.maxcpuunit IS E'unit for MAXCPUCOUNT';
COMMENT ON COLUMN jobsactive4.maxdiskcount IS E'expected disk size used by the job';
COMMENT ON COLUMN jobsactive4.maxdiskunit IS E'unit for MAXDISKCOUNT';
COMMENT ON COLUMN jobsactive4.maxpss IS E'Maximum PSS in the job';
COMMENT ON COLUMN jobsactive4.maxrss IS E'Maximum RSS in the job';
COMMENT ON COLUMN jobsactive4.maxswap IS E'Maximum SWAP in the job';
COMMENT ON COLUMN jobsactive4.maxvmem IS E'Maximum VMEM in the job';
COMMENT ON COLUMN jobsactive4.maxwalltime IS E'Estimated walltime limit for the job';
COMMENT ON COLUMN jobsactive4.memory_leak IS E'Memory leak in KB/s';
COMMENT ON COLUMN jobsactive4.memory_leak_x2 IS E'Memory leak square statistic';
COMMENT ON COLUMN jobsactive4.metadata IS E'unused. moved to ATLAS_PANDA.METATABLE. remains for code compatibility';
COMMENT ON COLUMN jobsactive4.minramcount IS E'expected amount of memory usage of the job';
COMMENT ON COLUMN jobsactive4.minramunit IS E'unit for MINRAMCOUNT';
COMMENT ON COLUMN jobsactive4.modificationhost IS E'the hostname which updates job status';
COMMENT ON COLUMN jobsactive4.modificationtime IS E'timestamp in UTC set by Panda server when job state changes or pilot update is received';
COMMENT ON COLUMN jobsactive4.nevents IS E'number of events processed by the job';
COMMENT ON COLUMN jobsactive4.ninputdatafiles IS E'the number of input files';
COMMENT ON COLUMN jobsactive4.ninputfiles IS E'the number of input files which the pilot processed on WN';
COMMENT ON COLUMN jobsactive4.noutputdatafiles IS E'the number of output files';
COMMENT ON COLUMN jobsactive4.nucleus IS E'Name of the site where the job is assigned in WORLD cloud';
COMMENT ON COLUMN jobsactive4.outputfilebytes IS E'the total size of output files';
COMMENT ON COLUMN jobsactive4.pandaid IS E'sequential ID generated from Oracle sequence object JOBSDEFINED4_PANDAID_SEQ when the job is inserted to jobsDefined4';
COMMENT ON COLUMN jobsactive4.parentid IS E'when the job is a retry for another job, PandaID of the retried job is set in this field';
COMMENT ON COLUMN jobsactive4.piloterrorcode IS E'pilot error code';
COMMENT ON COLUMN jobsactive4.piloterrordiag IS E'pilot error diagnostics';
COMMENT ON COLUMN jobsactive4.pilotid IS E'ID assigned to pilot by scheduler';
COMMENT ON COLUMN jobsactive4.pilottiming IS E'time consumption of the pilot';
COMMENT ON COLUMN jobsactive4.processingtype IS E'type of the job comes from etask.tasktype2';
COMMENT ON COLUMN jobsactive4.proddblock IS E'name of dataset containing job input files';
COMMENT ON COLUMN jobsactive4.proddbupdatetime IS E'timestamp of the last update in Oracle ProdDB';
COMMENT ON COLUMN jobsactive4.prodserieslabel IS E'constrained to DC2, Rome, DC3, pandatest';
COMMENT ON COLUMN jobsactive4.prodsourcelabel IS E'activity name of the name such as managed, user, and ddm';
COMMENT ON COLUMN jobsactive4.produserid IS E'ID of the user defined the job (user''s certificateDN or e-mail address)';
COMMENT ON COLUMN jobsactive4.produsername IS E'the name of the job submitter';
COMMENT ON COLUMN jobsactive4.raterbytes IS E'Read bytes rate';
COMMENT ON COLUMN jobsactive4.raterchar IS E'Read chars rate';
COMMENT ON COLUMN jobsactive4.ratewbytes IS E'Write bytes rate';
COMMENT ON COLUMN jobsactive4.ratewchar IS E'Write chars rate';
COMMENT ON COLUMN jobsactive4.relocationflag IS E'flag for submitting jobs to a single site. I.e. the brokerage is bypassed';
COMMENT ON COLUMN jobsactive4.resource_type IS E'Resource type name';
COMMENT ON COLUMN jobsactive4.schedulerid IS E'ID identifying the pilot scheduler';
COMMENT ON COLUMN jobsactive4.sourcesite IS E'source site (usually CE) for file transfer';
COMMENT ON COLUMN jobsactive4.specialhandling IS E'set when the job is specially handled in PanDA, such as re-brokerage';
COMMENT ON COLUMN jobsactive4.starttime IS E'start time when the job got started on WN';
COMMENT ON COLUMN jobsactive4.statechangetime IS E'timestamp of the last state change';
COMMENT ON COLUMN jobsactive4.superrorcode IS E'supervisor error code';
COMMENT ON COLUMN jobsactive4.superrordiag IS E'supervisor error diagnostics';
COMMENT ON COLUMN jobsactive4.taskbuffererrorcode IS E'taskBuffer error code';
COMMENT ON COLUMN jobsactive4.taskbuffererrordiag IS E'taskBuffer error diagnostics';
COMMENT ON COLUMN jobsactive4.taskid IS E'task ID defined in prodDB';
COMMENT ON COLUMN jobsactive4.totrbytes IS E'Total read bytes';
COMMENT ON COLUMN jobsactive4.totrchar IS E'Total read chars';
COMMENT ON COLUMN jobsactive4.totwbytes IS E'Total write bytes';
COMMENT ON COLUMN jobsactive4.totwchar IS E'Total write chars';
COMMENT ON COLUMN jobsactive4.transexitcode IS E'transformation exit code';
COMMENT ON COLUMN jobsactive4.transfertype IS E'type of file transfer';
COMMENT ON COLUMN jobsactive4.transformation IS E'Payload job script';
COMMENT ON COLUMN jobsactive4.vo IS E'Virtual Organization';
COMMENT ON COLUMN jobsactive4.workinggroup IS E'working group name';
ALTER TABLE jobsactive4 OWNER TO panda;
CREATE INDEX jobsactive4_compsitestatus_idx ON jobsactive4 (computingsite, jobstatus);
CREATE INDEX jobsactive4_csite_label_prior3 ON jobsactive4 (computingsite, prodsourcelabel, currentpriority, jobstatus, maxdiskcount, commandtopilot);
CREATE INDEX jobsactive4_jeditaskidstat_idx ON jobsactive4 (jeditaskid, jobstatus);
CREATE UNIQUE INDEX jobsactive4_jeditaskid_idx ON jobsactive4 (jeditaskid, pandaid);
CREATE INDEX jobsactive4_jobdefid_idx ON jobsactive4 (jobdefinitionid);
CREATE INDEX jobsactive4_jobname_idx ON jobsactive4 (jobname);
CREATE INDEX jobsactive4_modtime_idx ON jobsactive4 (modificationtime);
CREATE INDEX jobsactive4_prior_idx ON jobsactive4 (currentpriority, pandaid);
CREATE INDEX jobsactive4_proddblock_st_idx ON jobsactive4 (proddblock, jobstatus);
CREATE INDEX jobsactive4_produsernamest_idx ON jobsactive4 (produsername, jobstatus);
CREATE INDEX jobsactive4_reqid_idx ON jobsactive4 (reqid);
CREATE INDEX jobsactive4_stat_label_wgr_idx ON jobsactive4 (jobstatus, prodsourcelabel, workinggroup);
CREATE INDEX jobsactive4_workqueue_idx ON jobsactive4 (workqueue_id, cloud, jobstatus, prodsourcelabel, currentpriority);
CREATE INDEX jobsactive_statechangetime_idx ON jobsactive4 (statechangetime);
ALTER TABLE jobsactive4 ADD PRIMARY KEY (pandaid);

CREATE TABLE jobsarchived4 (
	pandaid bigint NOT NULL DEFAULT '0',
	modificationtime timestamp NOT NULL,
	jobdefinitionid bigint NOT NULL DEFAULT '0',
	schedulerid varchar(128),
	pilotid varchar(200),
	creationtime timestamp NOT NULL DEFAULT to_date('01-JAN-1970 00:00:00','DD-MON-YYYY HH24:MI:SS'),
	creationhost varchar(128),
	modificationhost varchar(128),
	atlasrelease varchar(64),
	transformation varchar(250),
	homepackage varchar(80),
	prodserieslabel varchar(20) DEFAULT 'Rome',
	prodsourcelabel varchar(20) DEFAULT 'managed',
	produserid varchar(250),
	assignedpriority integer NOT NULL DEFAULT '0',
	currentpriority integer NOT NULL DEFAULT '0',
	attemptnr smallint NOT NULL DEFAULT '0',
	maxattempt smallint NOT NULL DEFAULT '0',
	jobstatus varchar(15) NOT NULL DEFAULT 'activated',
	jobname varchar(256),
	maxcpucount bigint NOT NULL DEFAULT '0',
	maxcpuunit varchar(32),
	maxdiskcount bigint NOT NULL DEFAULT '0',
	maxdiskunit char(4),
	ipconnectivity char(5),
	minramcount bigint NOT NULL DEFAULT '0',
	minramunit char(2),
	starttime timestamp DEFAULT to_date('01-JAN-1970 00:00:00','DD-MON-YYYY HH24:MI:SS'),
	endtime timestamp DEFAULT to_date('01-JAN-1970 00:00:00','DD-MON-YYYY HH24:MI:SS'),
	cpuconsumptiontime numeric(20) NOT NULL DEFAULT '0',
	cpuconsumptionunit varchar(128),
	commandtopilot varchar(250),
	transexitcode varchar(128),
	piloterrorcode integer NOT NULL DEFAULT '0',
	piloterrordiag varchar(500),
	exeerrorcode integer NOT NULL DEFAULT '0',
	exeerrordiag varchar(500),
	superrorcode integer NOT NULL DEFAULT '0',
	superrordiag varchar(250),
	ddmerrorcode integer NOT NULL DEFAULT '0',
	ddmerrordiag varchar(700),
	brokerageerrorcode integer NOT NULL DEFAULT '0',
	brokerageerrordiag varchar(250),
	jobdispatchererrorcode integer NOT NULL DEFAULT '0',
	jobdispatchererrordiag varchar(250),
	taskbuffererrorcode integer NOT NULL DEFAULT '0',
	taskbuffererrordiag varchar(300),
	computingsite varchar(128),
	computingelement varchar(128),
	proddblock varchar(255),
	dispatchdblock varchar(255),
	destinationdblock varchar(255),
	destinationse varchar(250),
	nevents bigint NOT NULL DEFAULT '0',
	grid varchar(50),
	cloud varchar(50),
	cpuconversion decimal(9,4),
	sourcesite varchar(36),
	destinationsite varchar(36),
	transfertype varchar(10),
	taskid integer,
	cmtconfig varchar(250),
	statechangetime timestamp DEFAULT to_date('01-JAN-1970 00:00:00','DD-MON-YYYY HH24:MI:SS'),
	proddbupdatetime timestamp DEFAULT to_date('01-JAN-1970 00:00:00','DD-MON-YYYY HH24:MI:SS'),
	lockedby varchar(128),
	relocationflag smallint DEFAULT '0',
	jobexecutionid bigint DEFAULT '0',
	vo varchar(16),
	pilottiming varchar(100),
	workinggroup varchar(20),
	processingtype varchar(64),
	produsername varchar(60),
	ninputfiles integer,
	countrygroup varchar(20),
	batchid varchar(80),
	parentid bigint,
	specialhandling varchar(80),
	jobsetid bigint,
	corecount smallint,
	jobparameters text,
	metadata text,
	ninputdatafiles integer,
	inputfiletype varchar(32),
	inputfileproject varchar(64),
	inputfilebytes bigint,
	noutputdatafiles integer,
	outputfilebytes bigint,
	jobmetrics varchar(500),
	workqueue_id integer,
	jeditaskid bigint,
	jobsubstatus varchar(80),
	actualcorecount integer,
	reqid integer,
	maxrss bigint,
	maxvmem bigint,
	maxswap bigint,
	maxpss bigint,
	avgrss bigint,
	avgvmem bigint,
	avgswap bigint,
	avgpss bigint,
	maxwalltime bigint,
	nucleus varchar(52),
	eventservice smallint,
	failedattempt smallint,
	hs06sec bigint,
	gshare varchar(32),
	hs06 bigint,
	totrchar bigint,
	totwchar bigint,
	totrbytes bigint,
	totwbytes bigint,
	raterchar bigint,
	ratewchar bigint,
	raterbytes bigint,
	ratewbytes bigint,
	resource_type varchar(56),
	diskio integer,
	memory_leak bigint,
	memory_leak_x2 bigint,
	container_name varchar(200),
	job_label varchar(20),
	meancorecount decimal(8,2)
) PARTITION BY RANGE (modificationtime) ;
COMMENT ON TABLE jobsarchived4 IS E'Table for hosting all PanDA jobs that are in finished, failed or cancelled status. Data is regularly copied to an archive table in ATLAS_PANDAARCH schema. Data retention of the JOBSARCHIVED4 table is defined to be 3 days (can be changed if necessary). All timestamp and date type columns are in UTC ';
COMMENT ON COLUMN jobsarchived4.assignedpriority IS E'defined priority value';
COMMENT ON COLUMN jobsarchived4.atlasrelease IS E'Release required to run the job';
COMMENT ON COLUMN jobsarchived4.attemptnr IS E'how many times the job was retried so far';
COMMENT ON COLUMN jobsarchived4.avgpss IS E'Average PSS in the job';
COMMENT ON COLUMN jobsarchived4.avgrss IS E'AverageÂ RSS in the job';
COMMENT ON COLUMN jobsarchived4.avgswap IS E'Average SWAP in the job';
COMMENT ON COLUMN jobsarchived4.avgvmem IS E'Average VMEM in the job';
COMMENT ON COLUMN jobsarchived4.batchid IS E'ID of the job in the backend batch system';
COMMENT ON COLUMN jobsarchived4.brokerageerrorcode IS E'brokerage error code';
COMMENT ON COLUMN jobsarchived4.brokerageerrordiag IS E'brokerage error diagnostics';
COMMENT ON COLUMN jobsarchived4.cloud IS E'cloud (associated with Tier 1) where the job is submitted to';
COMMENT ON COLUMN jobsarchived4.cmtconfig IS E'cmt config';
COMMENT ON COLUMN jobsarchived4.commandtopilot IS E'used to send commands to pilot from Panda server, to kill the job for instance';
COMMENT ON COLUMN jobsarchived4.computingelement IS E'name of computing element';
COMMENT ON COLUMN jobsarchived4.computingsite IS E'site name where the job runs';
COMMENT ON COLUMN jobsarchived4.corecount IS E'the number of CPU cores';
COMMENT ON COLUMN jobsarchived4.countrygroup IS E'country of the job submitter';
COMMENT ON COLUMN jobsarchived4.cpuconsumptiontime IS E'actual execution time of the job';
COMMENT ON COLUMN jobsarchived4.cpuconsumptionunit IS E'unit for CPUCONSUMPTIONTIME';
COMMENT ON COLUMN jobsarchived4.cpuconversion IS E'CPU conversion factor';
COMMENT ON COLUMN jobsarchived4.creationhost IS E'the hostname where the job is submitted';
COMMENT ON COLUMN jobsarchived4.creationtime IS E'generated by Oracle''s SYSDATE function when the job is inserted to jobsDefined4';
COMMENT ON COLUMN jobsarchived4.currentpriority IS E'actual priority value which is usually the same as assignedPriority, can be modified by Panda server';
COMMENT ON COLUMN jobsarchived4.ddmerrorcode IS E'DDM error code';
COMMENT ON COLUMN jobsarchived4.ddmerrordiag IS E'DDM error diagnostics';
COMMENT ON COLUMN jobsarchived4.destinationdblock IS E'name of destination dataset for the job; is used to register the outputs of an associated set of jobs as belonging to one block to be saved at an archival destination';
COMMENT ON COLUMN jobsarchived4.destinationse IS E'destination storage element of job output files';
COMMENT ON COLUMN jobsarchived4.destinationsite IS E'destination site (usually SE) for file transfer';
COMMENT ON COLUMN jobsarchived4.diskio IS E'Local disk access in kBPerSec. Required to limit the number of running jobs based on total IO for each queue.';
COMMENT ON COLUMN jobsarchived4.dispatchdblock IS E'name of dispatch dataset for the job ; a prodDBlock may be broken down into smaller blocks for dispatch to sites';
COMMENT ON COLUMN jobsarchived4.endtime IS E'end time when the job finished on WN';
COMMENT ON COLUMN jobsarchived4.eventservice IS E'The job uses Event Service';
COMMENT ON COLUMN jobsarchived4.exeerrorcode IS E'executor error code';
COMMENT ON COLUMN jobsarchived4.exeerrordiag IS E'executor error diagnostics';
COMMENT ON COLUMN jobsarchived4.failedattempt IS E'How many times the input files were failed so far. The maximum number is used if there are multiple input files';
COMMENT ON COLUMN jobsarchived4.grid IS E'GRID where the job is submitted to';
COMMENT ON COLUMN jobsarchived4.gshare IS E'Global share';
COMMENT ON COLUMN jobsarchived4.homepackage IS E'Cache required to run the job';
COMMENT ON COLUMN jobsarchived4.hs06 IS E'Core count x core power';
COMMENT ON COLUMN jobsarchived4.hs06sec IS E'The product of HS06 score and CPU consumption time';
COMMENT ON COLUMN jobsarchived4.inputfilebytes IS E'the total size of input files';
COMMENT ON COLUMN jobsarchived4.inputfileproject IS E'project name of input files';
COMMENT ON COLUMN jobsarchived4.inputfiletype IS E'type of input files';
COMMENT ON COLUMN jobsarchived4.ipconnectivity IS E'defined in prodDB (unused in Panda)';
COMMENT ON COLUMN jobsarchived4.jobdefinitionid IS E'comes from ejobdef.jobdefid for managed production jobs, and defines job set IDs for analysis jobs';
COMMENT ON COLUMN jobsarchived4.jobdispatchererrorcode IS E'jobDispatcher error code';
COMMENT ON COLUMN jobsarchived4.jobdispatchererrordiag IS E'jobDispatcher error diagnostics';
COMMENT ON COLUMN jobsarchived4.jobexecutionid IS E'job execution ID coming from ProdDB';
COMMENT ON COLUMN jobsarchived4.jobmetrics IS E'a general-purpose field to record various job metrics';
COMMENT ON COLUMN jobsarchived4.jobname IS E'the job name defined in prodDB';
COMMENT ON COLUMN jobsarchived4.jobparameters IS E'unused. moved to ATLAS_PANDA.JOBPARAMSTABLE. remains for code compatibility';
COMMENT ON COLUMN jobsarchived4.jobsetid IS E'jobset ID for analysis jobs';
COMMENT ON COLUMN jobsarchived4.jobstatus IS E'status of the job';
COMMENT ON COLUMN jobsarchived4.lockedby IS E'the name of the current writer of the record in ProdDB';
COMMENT ON COLUMN jobsarchived4.maxattempt IS E'how many times the job can be retried';
COMMENT ON COLUMN jobsarchived4.maxcpucount IS E'expected execution time of the job';
COMMENT ON COLUMN jobsarchived4.maxcpuunit IS E'unit for MAXCPUCOUNT';
COMMENT ON COLUMN jobsarchived4.maxdiskcount IS E'expected disk size used by the job';
COMMENT ON COLUMN jobsarchived4.maxdiskunit IS E'unit for MAXDISKCOUNT';
COMMENT ON COLUMN jobsarchived4.maxpss IS E'Maximum PSS in the job';
COMMENT ON COLUMN jobsarchived4.maxrss IS E'Maximum RSS in the job';
COMMENT ON COLUMN jobsarchived4.maxswap IS E'Maximum SWAP in the job';
COMMENT ON COLUMN jobsarchived4.maxvmem IS E'Maximum VMEM in the job';
COMMENT ON COLUMN jobsarchived4.maxwalltime IS E'Estimated walltime limit for the job';
COMMENT ON COLUMN jobsarchived4.memory_leak IS E'Memory leak in KB/s';
COMMENT ON COLUMN jobsarchived4.memory_leak_x2 IS E'Memory leak square statistic';
COMMENT ON COLUMN jobsarchived4.metadata IS E'unused. moved to ATLAS_PANDA.METATABLE. remains for code compatibility';
COMMENT ON COLUMN jobsarchived4.minramcount IS E'expected amount of memory usage of the job';
COMMENT ON COLUMN jobsarchived4.minramunit IS E'unit for MINRAMCOUNT';
COMMENT ON COLUMN jobsarchived4.modificationhost IS E'the hostname which updates job status';
COMMENT ON COLUMN jobsarchived4.modificationtime IS E'timestamp in UTC set by Panda server when job state changes or pilot update is received';
COMMENT ON COLUMN jobsarchived4.nevents IS E'number of events processed by the job';
COMMENT ON COLUMN jobsarchived4.ninputdatafiles IS E'the number of input files';
COMMENT ON COLUMN jobsarchived4.ninputfiles IS E'the number of input files which the pilot processed on WN';
COMMENT ON COLUMN jobsarchived4.noutputdatafiles IS E'the number of output files';
COMMENT ON COLUMN jobsarchived4.nucleus IS E'Name of the site where the job is assigned in WORLD cloud';
COMMENT ON COLUMN jobsarchived4.outputfilebytes IS E'the total size of output files';
COMMENT ON COLUMN jobsarchived4.pandaid IS E'sequential ID generated from Oracle sequence object JOBSDEFINED4_PANDAID_SEQ when the job is inserted to jobsDefined4';
COMMENT ON COLUMN jobsarchived4.parentid IS E'when the job is a retry for another job, PandaID of the retried job is set in this field';
COMMENT ON COLUMN jobsarchived4.piloterrorcode IS E'pilot error code';
COMMENT ON COLUMN jobsarchived4.piloterrordiag IS E'pilot error diagnostics';
COMMENT ON COLUMN jobsarchived4.pilotid IS E'ID assigned to pilot by scheduler';
COMMENT ON COLUMN jobsarchived4.pilottiming IS E'time consumption of the pilot';
COMMENT ON COLUMN jobsarchived4.processingtype IS E'type of the job comes from etask.tasktype2';
COMMENT ON COLUMN jobsarchived4.proddblock IS E'name of dataset containing job input files';
COMMENT ON COLUMN jobsarchived4.proddbupdatetime IS E'timestamp of the last update in Oracle ProdDB';
COMMENT ON COLUMN jobsarchived4.prodserieslabel IS E'constrained to DC2, Rome, DC3, pandatest';
COMMENT ON COLUMN jobsarchived4.prodsourcelabel IS E'activity name of the name such as managed, user, and ddm';
COMMENT ON COLUMN jobsarchived4.produserid IS E'ID of the user defined the job (user''s certificateDN or e-mail address)';
COMMENT ON COLUMN jobsarchived4.produsername IS E'the name of the job submitter';
COMMENT ON COLUMN jobsarchived4.raterbytes IS E'Read bytes rate';
COMMENT ON COLUMN jobsarchived4.raterchar IS E'Read chars rate';
COMMENT ON COLUMN jobsarchived4.ratewbytes IS E'Write bytes rate';
COMMENT ON COLUMN jobsarchived4.ratewchar IS E'Write chars rate';
COMMENT ON COLUMN jobsarchived4.relocationflag IS E'flag for submitting jobs to a single site. I.e. the brokerage is bypassed';
COMMENT ON COLUMN jobsarchived4.resource_type IS E'Resource type name';
COMMENT ON COLUMN jobsarchived4.schedulerid IS E'ID identifying the pilot scheduler';
COMMENT ON COLUMN jobsarchived4.sourcesite IS E'source site (usually CE) for file transfer';
COMMENT ON COLUMN jobsarchived4.specialhandling IS E'set when the job is specially handled in PanDA, such as re-brokerage';
COMMENT ON COLUMN jobsarchived4.starttime IS E'start time when the job got started on WN';
COMMENT ON COLUMN jobsarchived4.statechangetime IS E'timestamp of the last state change';
COMMENT ON COLUMN jobsarchived4.superrorcode IS E'supervisor error code';
COMMENT ON COLUMN jobsarchived4.superrordiag IS E'supervisor error diagnostics';
COMMENT ON COLUMN jobsarchived4.taskbuffererrorcode IS E'taskBuffer error code';
COMMENT ON COLUMN jobsarchived4.taskbuffererrordiag IS E'taskBuffer error diagnostics';
COMMENT ON COLUMN jobsarchived4.taskid IS E'task ID defined in prodDB';
COMMENT ON COLUMN jobsarchived4.totrbytes IS E'Total read bytes';
COMMENT ON COLUMN jobsarchived4.totrchar IS E'Total read chars';
COMMENT ON COLUMN jobsarchived4.totwbytes IS E'Total write bytes';
COMMENT ON COLUMN jobsarchived4.totwchar IS E'Total write chars';
COMMENT ON COLUMN jobsarchived4.transexitcode IS E'transformation exit code';
COMMENT ON COLUMN jobsarchived4.transfertype IS E'type of file transfer';
COMMENT ON COLUMN jobsarchived4.transformation IS E'Payload job script';
COMMENT ON COLUMN jobsarchived4.vo IS E'Virtual Organization';
COMMENT ON COLUMN jobsarchived4.workinggroup IS E'working group name';
ALTER TABLE jobsarchived4 OWNER TO panda;
CREATE INDEX jobsarc4_jeditaskid3attr_idx ON jobsarchived4 (jeditaskid, jobstatus, prodsourcelabel, pandaid);
CREATE INDEX jobsarch4_compsite_5attr_idx ON jobsarchived4 (computingsite, cloud, jobstatus, prodsourcelabel, processingtype, modificationtime);
CREATE INDEX jobsarch4_mtimeprodslabel_idx ON jobsarchived4 (modificationtime, prodsourcelabel);
CREATE INDEX jobsarch4_piloterrcode_idx ON jobsarchived4 (piloterrorcode);
CREATE INDEX jobsarch4_produsernamest_idx ON jobsarchived4 (produsername, jobstatus);
CREATE INDEX jobsarch4_specialhandling_idx ON jobsarchived4 (specialhandling);
CREATE INDEX jobsarch4_taskid_3attr_idx ON jobsarchived4 (taskid, prodsourcelabel, jobstatus, processingtype);
CREATE INDEX jobsarch4_workgroup_status_idx ON jobsarchived4 (workinggroup, jobstatus);
CREATE INDEX jobsarchived4_batchid_idx ON jobsarchived4 (batchid);
CREATE INDEX jobsarchived4_changetime ON jobsarchived4 (lockedby, prodsourcelabel, (case when statechangetime>proddbupdatetime then 1 else null end));
CREATE INDEX jobsarchived4_jobdefid_idx ON jobsarchived4 (jobdefinitionid);
CREATE INDEX jobsarchived4_jobname_idx ON jobsarchived4 (jobname);
CREATE INDEX jobsarchived4_jobsetid_idx ON jobsarchived4 (jobsetid);
CREATE INDEX jobsarchived4_reqid_idx ON jobsarchived4 (reqid);
CREATE INDEX jobsarchived4_workqueue_idx ON jobsarchived4 (workqueue_id, cloud, jobstatus, prodsourcelabel, currentpriority);
CREATE INDEX jobs_destinationdblock_idx ON jobsarchived4 (destinationdblock);
CREATE INDEX jobs_statechangetime_idx ON jobsarchived4 (statechangetime);
CREATE INDEX jobs_upper_produsername_idx ON jobsarchived4 (upper(produsername));
ALTER TABLE jobsarchived4 ADD PRIMARY KEY (pandaid,modificationtime);

CREATE TABLE jobsdebug (
	pandaid bigint NOT NULL,
	stdout varchar(2048)
) ;
COMMENT ON TABLE jobsdebug IS E'Table with job stdout which is sent by the pilot on the worker node in order to enable real-time monitoring of job status';
COMMENT ON COLUMN jobsdebug.pandaid IS E'PandaID of the job';
COMMENT ON COLUMN jobsdebug.stdout IS E'snippet of stdout sent by the pilot';
ALTER TABLE jobsdebug OWNER TO panda;
ALTER TABLE jobsdebug ADD PRIMARY KEY (pandaid);

CREATE TABLE jobsdefined4 (
	pandaid bigint NOT NULL DEFAULT '0',
	jobdefinitionid bigint NOT NULL DEFAULT '0',
	schedulerid varchar(128),
	pilotid varchar(200),
	creationtime timestamp NOT NULL DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	creationhost varchar(128),
	modificationtime timestamp NOT NULL DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	modificationhost varchar(128),
	atlasrelease varchar(64),
	transformation varchar(250),
	homepackage varchar(80),
	prodserieslabel varchar(20) DEFAULT 'Rome',
	prodsourcelabel varchar(20) DEFAULT 'managed',
	produserid varchar(250),
	assignedpriority integer NOT NULL DEFAULT '0',
	currentpriority integer NOT NULL DEFAULT '0',
	attemptnr smallint NOT NULL DEFAULT '0',
	maxattempt smallint NOT NULL DEFAULT '0',
	jobstatus varchar(15) NOT NULL DEFAULT 'defined',
	jobname varchar(256),
	maxcpucount integer NOT NULL DEFAULT '0',
	maxcpuunit varchar(32),
	maxdiskcount integer NOT NULL DEFAULT '0',
	maxdiskunit char(4),
	ipconnectivity char(5),
	minramcount integer NOT NULL DEFAULT '0',
	minramunit char(2),
	starttime timestamp DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	endtime timestamp DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	cpuconsumptiontime numeric(20) NOT NULL DEFAULT '0',
	cpuconsumptionunit varchar(128),
	commandtopilot varchar(250),
	transexitcode varchar(128),
	piloterrorcode integer NOT NULL DEFAULT '0',
	piloterrordiag varchar(250),
	exeerrorcode integer NOT NULL DEFAULT '0',
	exeerrordiag varchar(250),
	superrorcode integer NOT NULL DEFAULT '0',
	superrordiag varchar(250),
	ddmerrorcode integer NOT NULL DEFAULT '0',
	ddmerrordiag varchar(250),
	brokerageerrorcode integer NOT NULL DEFAULT '0',
	brokerageerrordiag varchar(250),
	jobdispatchererrorcode integer NOT NULL DEFAULT '0',
	jobdispatchererrordiag varchar(250),
	taskbuffererrorcode integer NOT NULL DEFAULT '0',
	taskbuffererrordiag varchar(250),
	computingsite varchar(128),
	computingelement varchar(128),
	jobparameters text,
	metadata text,
	proddblock varchar(255),
	dispatchdblock varchar(255),
	destinationdblock varchar(255),
	destinationse varchar(250),
	nevents integer NOT NULL DEFAULT '0',
	grid varchar(32),
	cloud varchar(32),
	cpuconversion decimal(9,4),
	sourcesite varchar(36),
	destinationsite varchar(36),
	transfertype varchar(10),
	taskid integer,
	cmtconfig varchar(250),
	statechangetime timestamp DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	proddbupdatetime timestamp DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	lockedby varchar(128),
	relocationflag smallint DEFAULT '0',
	jobexecutionid bigint DEFAULT '0',
	vo varchar(16),
	pilottiming varchar(100),
	workinggroup varchar(20),
	processingtype varchar(64),
	produsername varchar(60),
	ninputfiles integer,
	countrygroup varchar(20),
	batchid varchar(80),
	parentid bigint,
	specialhandling varchar(80),
	jobsetid bigint,
	corecount smallint,
	ninputdatafiles integer,
	inputfiletype varchar(32),
	inputfileproject varchar(64),
	inputfilebytes bigint,
	noutputdatafiles integer,
	outputfilebytes bigint,
	jobmetrics varchar(500),
	workqueue_id integer,
	jeditaskid bigint,
	jobsubstatus varchar(80),
	actualcorecount integer,
	reqid integer,
	maxrss bigint,
	maxvmem bigint,
	maxswap bigint,
	maxpss bigint,
	avgrss bigint,
	avgvmem bigint,
	avgswap bigint,
	avgpss bigint,
	maxwalltime bigint,
	nucleus varchar(52),
	eventservice smallint,
	failedattempt smallint,
	hs06sec bigint,
	gshare varchar(32),
	hs06 bigint,
	totrchar bigint,
	totwchar bigint,
	totrbytes bigint,
	totwbytes bigint,
	raterchar bigint,
	ratewchar bigint,
	raterbytes bigint,
	ratewbytes bigint,
	resource_type varchar(56),
	diskio integer,
	memory_leak bigint,
	memory_leak_x2 bigint,
	container_name varchar(200),
	job_label varchar(20),
	meancorecount decimal(8,2)
) ;
COMMENT ON COLUMN jobsdefined4.assignedpriority IS E'defined priority value';
COMMENT ON COLUMN jobsdefined4.atlasrelease IS E'Release required to run the job';
COMMENT ON COLUMN jobsdefined4.attemptnr IS E'how many times the job was retried so far';
COMMENT ON COLUMN jobsdefined4.avgpss IS E'Average PSS in the job';
COMMENT ON COLUMN jobsdefined4.avgrss IS E'AverageÂ RSS in the job';
COMMENT ON COLUMN jobsdefined4.avgswap IS E'Average SWAP in the job';
COMMENT ON COLUMN jobsdefined4.avgvmem IS E'Average VMEM in the job';
COMMENT ON COLUMN jobsdefined4.batchid IS E'ID of the job in the backend batch system';
COMMENT ON COLUMN jobsdefined4.brokerageerrorcode IS E'brokerage error code';
COMMENT ON COLUMN jobsdefined4.brokerageerrordiag IS E'brokerage error diagnostics';
COMMENT ON COLUMN jobsdefined4.cloud IS E'cloud (associated with Tier 1) where the job is submitted to';
COMMENT ON COLUMN jobsdefined4.cmtconfig IS E'cmt config';
COMMENT ON COLUMN jobsdefined4.commandtopilot IS E'used to send commands to pilot from Panda server, to kill the job for instance';
COMMENT ON COLUMN jobsdefined4.computingelement IS E'name of computing element';
COMMENT ON COLUMN jobsdefined4.computingsite IS E'site name where the job runs';
COMMENT ON COLUMN jobsdefined4.corecount IS E'the number of CPU cores';
COMMENT ON COLUMN jobsdefined4.countrygroup IS E'country of the job submitter';
COMMENT ON COLUMN jobsdefined4.cpuconsumptiontime IS E'actual execution time of the job';
COMMENT ON COLUMN jobsdefined4.cpuconsumptionunit IS E'unit for CPUCONSUMPTIONTIME';
COMMENT ON COLUMN jobsdefined4.cpuconversion IS E'CPU conversion factor';
COMMENT ON COLUMN jobsdefined4.creationhost IS E'the hostname where the job is submitted';
COMMENT ON COLUMN jobsdefined4.creationtime IS E'generated by Oracle''s SYSDATE function when the job is inserted to jobsDefined4';
COMMENT ON COLUMN jobsdefined4.currentpriority IS E'actual priority value which is usually the same as assignedPriority, can be modified by Panda server';
COMMENT ON COLUMN jobsdefined4.ddmerrorcode IS E'DDM error code';
COMMENT ON COLUMN jobsdefined4.ddmerrordiag IS E'DDM error diagnostics';
COMMENT ON COLUMN jobsdefined4.destinationdblock IS E'name of destination dataset for the job; is used to register the outputs of an associated set of jobs as belonging to one block to be saved at an archival destination';
COMMENT ON COLUMN jobsdefined4.destinationse IS E'destination storage element of job output files';
COMMENT ON COLUMN jobsdefined4.destinationsite IS E'destination site (usually SE) for file transfer';
COMMENT ON COLUMN jobsdefined4.diskio IS E'Local disk access in kBPerSec. Required to limit the number of running jobs based on total IO for each queue.';
COMMENT ON COLUMN jobsdefined4.dispatchdblock IS E'name of dispatch dataset for the job ; a prodDBlock may be broken down into smaller blocks for dispatch to sites';
COMMENT ON COLUMN jobsdefined4.endtime IS E'end time when the job finished on WN';
COMMENT ON COLUMN jobsdefined4.eventservice IS E'The job uses Event Service';
COMMENT ON COLUMN jobsdefined4.exeerrorcode IS E'executor error code';
COMMENT ON COLUMN jobsdefined4.exeerrordiag IS E'executor error diagnostics';
COMMENT ON COLUMN jobsdefined4.failedattempt IS E'How many times the input files were failed so far. The maximum number is used if there are multiple input files';
COMMENT ON COLUMN jobsdefined4.grid IS E'GRID where the job is submitted to';
COMMENT ON COLUMN jobsdefined4.gshare IS E'Global share';
COMMENT ON COLUMN jobsdefined4.homepackage IS E'Cache required to run the job';
COMMENT ON COLUMN jobsdefined4.hs06 IS E'Core count x core power';
COMMENT ON COLUMN jobsdefined4.hs06sec IS E'The product of HS06 score and CPU consumption time';
COMMENT ON COLUMN jobsdefined4.inputfilebytes IS E'the total size of input files';
COMMENT ON COLUMN jobsdefined4.inputfileproject IS E'project name of input files';
COMMENT ON COLUMN jobsdefined4.inputfiletype IS E'type of input files';
COMMENT ON COLUMN jobsdefined4.ipconnectivity IS E'defined in prodDB (unused in Panda)';
COMMENT ON COLUMN jobsdefined4.jobdefinitionid IS E'comes from ejobdef.jobdefid for managed production jobs, and defines job set IDs for analysis jobs';
COMMENT ON COLUMN jobsdefined4.jobdispatchererrorcode IS E'jobDispatcher error code';
COMMENT ON COLUMN jobsdefined4.jobdispatchererrordiag IS E'jobDispatcher error diagnostics';
COMMENT ON COLUMN jobsdefined4.jobexecutionid IS E'job execution ID coming from ProdDB';
COMMENT ON COLUMN jobsdefined4.jobmetrics IS E'a general-purpose field to record various job metrics';
COMMENT ON COLUMN jobsdefined4.jobname IS E'the job name defined in prodDB';
COMMENT ON COLUMN jobsdefined4.jobparameters IS E'unused. moved to ATLAS_PANDA.JOBPARAMSTABLE. remains for code compatibility';
COMMENT ON COLUMN jobsdefined4.jobsetid IS E'jobset ID for analysis jobs';
COMMENT ON COLUMN jobsdefined4.jobstatus IS E'status of the job';
COMMENT ON COLUMN jobsdefined4.lockedby IS E'the name of the current writer of the record in ProdDB';
COMMENT ON COLUMN jobsdefined4.maxattempt IS E'how many times the job can be retried';
COMMENT ON COLUMN jobsdefined4.maxcpucount IS E'expected execution time of the job';
COMMENT ON COLUMN jobsdefined4.maxcpuunit IS E'unit for MAXCPUCOUNT';
COMMENT ON COLUMN jobsdefined4.maxdiskcount IS E'expected disk size used by the job';
COMMENT ON COLUMN jobsdefined4.maxdiskunit IS E'unit for MAXDISKCOUNT';
COMMENT ON COLUMN jobsdefined4.maxpss IS E'Maximum PSS in the job';
COMMENT ON COLUMN jobsdefined4.maxrss IS E'Maximum RSS in the job';
COMMENT ON COLUMN jobsdefined4.maxswap IS E'Maximum SWAP in the job';
COMMENT ON COLUMN jobsdefined4.maxvmem IS E'Maximum VMEM in the job';
COMMENT ON COLUMN jobsdefined4.maxwalltime IS E'Estimated walltime limit for the job';
COMMENT ON COLUMN jobsdefined4.memory_leak IS E'Memory leak in KB/s';
COMMENT ON COLUMN jobsdefined4.memory_leak_x2 IS E'Memory leak square statistic';
COMMENT ON COLUMN jobsdefined4.metadata IS E'unused. moved to ATLAS_PANDA.METATABLE. remains for code compatibility';
COMMENT ON COLUMN jobsdefined4.minramcount IS E'expected amount of memory usage of the job';
COMMENT ON COLUMN jobsdefined4.minramunit IS E'unit for MINRAMCOUNT';
COMMENT ON COLUMN jobsdefined4.modificationhost IS E'the hostname which updates job status';
COMMENT ON COLUMN jobsdefined4.modificationtime IS E'timestamp in UTC set by Panda server when job state changes or pilot update is received';
COMMENT ON COLUMN jobsdefined4.nevents IS E'number of events processed by the job';
COMMENT ON COLUMN jobsdefined4.ninputdatafiles IS E'the number of input files';
COMMENT ON COLUMN jobsdefined4.ninputfiles IS E'the number of input files which the pilot processed on WN';
COMMENT ON COLUMN jobsdefined4.noutputdatafiles IS E'the number of output files';
COMMENT ON COLUMN jobsdefined4.nucleus IS E'Name of the site where the job is assigned in WORLD cloud';
COMMENT ON COLUMN jobsdefined4.outputfilebytes IS E'the total size of output files';
COMMENT ON COLUMN jobsdefined4.pandaid IS E'sequential ID generated from Oracle sequence object JOBSDEFINED4_PANDAID_SEQ when the job is inserted to jobsDefined4';
COMMENT ON COLUMN jobsdefined4.parentid IS E'when the job is a retry for another job, PandaID of the retried job is set in this field';
COMMENT ON COLUMN jobsdefined4.piloterrorcode IS E'pilot error code';
COMMENT ON COLUMN jobsdefined4.piloterrordiag IS E'pilot error diagnostics';
COMMENT ON COLUMN jobsdefined4.pilotid IS E'ID assigned to pilot by scheduler';
COMMENT ON COLUMN jobsdefined4.pilottiming IS E'time consumption of the pilot';
COMMENT ON COLUMN jobsdefined4.processingtype IS E'type of the job comes from etask.tasktype2';
COMMENT ON COLUMN jobsdefined4.proddblock IS E'name of dataset containing job input files';
COMMENT ON COLUMN jobsdefined4.proddbupdatetime IS E'timestamp of the last update in Oracle ProdDB';
COMMENT ON COLUMN jobsdefined4.prodserieslabel IS E'constrained to DC2, Rome, DC3, pandatest';
COMMENT ON COLUMN jobsdefined4.prodsourcelabel IS E'activity name of the name such as managed, user, and ddm';
COMMENT ON COLUMN jobsdefined4.produserid IS E'ID of the user defined the job (user''s certificateDN or e-mail address)';
COMMENT ON COLUMN jobsdefined4.produsername IS E'the name of the job submitter';
COMMENT ON COLUMN jobsdefined4.raterbytes IS E'Read bytes rate';
COMMENT ON COLUMN jobsdefined4.raterchar IS E'Read chars rate';
COMMENT ON COLUMN jobsdefined4.ratewbytes IS E'Write bytes rate';
COMMENT ON COLUMN jobsdefined4.ratewchar IS E'Write chars rate';
COMMENT ON COLUMN jobsdefined4.relocationflag IS E'flag for submitting jobs to a single site. I.e. the brokerage is bypassed';
COMMENT ON COLUMN jobsdefined4.resource_type IS E'Resource type name';
COMMENT ON COLUMN jobsdefined4.schedulerid IS E'ID identifying the pilot scheduler';
COMMENT ON COLUMN jobsdefined4.sourcesite IS E'source site (usually CE) for file transfer';
COMMENT ON COLUMN jobsdefined4.specialhandling IS E'set when the job is specially handled in PanDA, such as re-brokerage';
COMMENT ON COLUMN jobsdefined4.starttime IS E'start time when the job got started on WN';
COMMENT ON COLUMN jobsdefined4.statechangetime IS E'timestamp of the last state change';
COMMENT ON COLUMN jobsdefined4.superrorcode IS E'supervisor error code';
COMMENT ON COLUMN jobsdefined4.superrordiag IS E'supervisor error diagnostics';
COMMENT ON COLUMN jobsdefined4.taskbuffererrorcode IS E'taskBuffer error code';
COMMENT ON COLUMN jobsdefined4.taskbuffererrordiag IS E'taskBuffer error diagnostics';
COMMENT ON COLUMN jobsdefined4.taskid IS E'task ID defined in prodDB';
COMMENT ON COLUMN jobsdefined4.totrbytes IS E'Total read bytes';
COMMENT ON COLUMN jobsdefined4.totrchar IS E'Total read chars';
COMMENT ON COLUMN jobsdefined4.totwbytes IS E'Total write bytes';
COMMENT ON COLUMN jobsdefined4.totwchar IS E'Total write chars';
COMMENT ON COLUMN jobsdefined4.transexitcode IS E'transformation exit code';
COMMENT ON COLUMN jobsdefined4.transfertype IS E'type of file transfer';
COMMENT ON COLUMN jobsdefined4.transformation IS E'Payload job script';
COMMENT ON COLUMN jobsdefined4.vo IS E'Virtual Organization';
COMMENT ON COLUMN jobsdefined4.workinggroup IS E'working group name';
ALTER TABLE jobsdefined4 OWNER TO panda;
CREATE UNIQUE INDEX jobsdefined4_jeditaskid_idx ON jobsdefined4 (jeditaskid, pandaid);
CREATE INDEX jobsdefined4_jobname_idx ON jobsdefined4 (jobname);
CREATE INDEX jobsdefined4_jobsetid_idx ON jobsdefined4 (jobsetid);
CREATE INDEX jobsdefined4_label_csite_stat ON jobsdefined4 (prodsourcelabel, computingsite, jobstatus);
CREATE INDEX jobsdefined4_produsername_idx ON jobsdefined4 (produsername);
CREATE INDEX jobsdefined4_reqid_idx ON jobsdefined4 (reqid);
CREATE INDEX jobsdefined4_workqueue_idx ON jobsdefined4 (workqueue_id, cloud, jobstatus, prodsourcelabel, currentpriority);
CREATE INDEX jobsdef_statechangetime_idx ON jobsdefined4 (statechangetime);
ALTER TABLE jobsdefined4 ADD PRIMARY KEY (pandaid);

CREATE TABLE jobsdefined_share_stats (
	ts timestamp,
	gshare varchar(32),
	computingsite varchar(128),
	jobstatus varchar(15),
	maxpriority bigint,
	prorated_diskio_avg decimal(11,2),
	njobs bigint,
	hs bigint,
	vo varchar(16),
	workqueue_id integer,
	resource_type varchar(56)
) ;
ALTER TABLE jobsdefined_share_stats OWNER TO panda;

CREATE TABLE jobswaiting4 (
	pandaid bigint NOT NULL DEFAULT '0',
	jobdefinitionid bigint NOT NULL DEFAULT '0',
	schedulerid varchar(128),
	pilotid varchar(200),
	creationtime timestamp NOT NULL DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	creationhost varchar(128),
	modificationtime timestamp NOT NULL DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	modificationhost varchar(128),
	atlasrelease varchar(64),
	transformation varchar(250),
	homepackage varchar(80),
	prodserieslabel varchar(20) DEFAULT 'Rome',
	prodsourcelabel varchar(20) DEFAULT 'managed',
	produserid varchar(250),
	assignedpriority integer NOT NULL DEFAULT '0',
	currentpriority integer NOT NULL DEFAULT '0',
	attemptnr smallint NOT NULL DEFAULT '0',
	maxattempt smallint NOT NULL DEFAULT '0',
	jobstatus varchar(15) NOT NULL DEFAULT 'waiting',
	jobname varchar(256),
	maxcpucount integer NOT NULL DEFAULT '0',
	maxcpuunit varchar(32),
	maxdiskcount integer NOT NULL DEFAULT '0',
	maxdiskunit char(4),
	ipconnectivity char(5),
	minramcount integer NOT NULL DEFAULT '0',
	minramunit char(2),
	starttime timestamp DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	endtime timestamp DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	cpuconsumptiontime numeric(20) NOT NULL DEFAULT '0',
	cpuconsumptionunit varchar(128),
	commandtopilot varchar(250),
	transexitcode varchar(128),
	piloterrorcode integer NOT NULL DEFAULT '0',
	piloterrordiag varchar(250),
	exeerrorcode integer NOT NULL DEFAULT '0',
	exeerrordiag varchar(250),
	superrorcode integer NOT NULL DEFAULT '0',
	superrordiag varchar(250),
	ddmerrorcode integer NOT NULL DEFAULT '0',
	ddmerrordiag varchar(250),
	brokerageerrorcode integer NOT NULL DEFAULT '0',
	brokerageerrordiag varchar(250),
	jobdispatchererrorcode integer NOT NULL DEFAULT '0',
	jobdispatchererrordiag varchar(250),
	taskbuffererrorcode integer NOT NULL DEFAULT '0',
	taskbuffererrordiag varchar(250),
	computingsite varchar(128),
	computingelement varchar(128),
	jobparameters text,
	metadata text,
	proddblock varchar(255),
	dispatchdblock varchar(255),
	destinationdblock varchar(255),
	destinationse varchar(250),
	nevents integer NOT NULL DEFAULT '0',
	grid varchar(32),
	cloud varchar(32),
	cpuconversion decimal(9,4),
	sourcesite varchar(36),
	destinationsite varchar(36),
	transfertype varchar(10),
	taskid integer,
	cmtconfig varchar(250),
	statechangetime timestamp DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	proddbupdatetime timestamp DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	lockedby varchar(128),
	relocationflag smallint DEFAULT '0',
	jobexecutionid bigint DEFAULT '0',
	vo varchar(16),
	pilottiming varchar(100),
	workinggroup varchar(20),
	processingtype varchar(64),
	produsername varchar(60),
	ninputfiles integer,
	countrygroup varchar(20),
	batchid varchar(80),
	parentid bigint,
	specialhandling varchar(80),
	jobsetid bigint,
	corecount smallint,
	ninputdatafiles integer,
	inputfiletype varchar(32),
	inputfileproject varchar(64),
	inputfilebytes bigint,
	noutputdatafiles integer,
	outputfilebytes bigint,
	jobmetrics varchar(500),
	workqueue_id integer,
	jeditaskid bigint,
	jobsubstatus varchar(80),
	actualcorecount integer,
	reqid integer,
	maxrss bigint,
	maxvmem bigint,
	maxswap bigint,
	maxpss bigint,
	avgrss bigint,
	avgvmem bigint,
	avgswap bigint,
	avgpss bigint,
	maxwalltime bigint,
	nucleus varchar(52),
	eventservice smallint,
	failedattempt smallint,
	hs06sec bigint,
	gshare varchar(32),
	hs06 bigint,
	totrchar bigint,
	totwchar bigint,
	totrbytes bigint,
	totwbytes bigint,
	raterchar bigint,
	ratewchar bigint,
	raterbytes bigint,
	ratewbytes bigint,
	resource_type varchar(56),
	diskio integer,
	memory_leak bigint,
	memory_leak_x2 bigint,
	container_name varchar(200),
	job_label varchar(20),
	meancorecount decimal(8,2)
) ;
COMMENT ON TABLE jobswaiting4 IS E'Table for hosting all PanDA jobs that are in waiting state. The columns (and their description) are the same as the ones in JOBSACTIVE4 table. All timestamp and date type columns are in UTC';
COMMENT ON COLUMN jobswaiting4.assignedpriority IS E'defined priority value';
COMMENT ON COLUMN jobswaiting4.atlasrelease IS E'Release required to run the job';
COMMENT ON COLUMN jobswaiting4.attemptnr IS E'how many times the job was retried so far';
COMMENT ON COLUMN jobswaiting4.avgpss IS E'Average PSS in the job';
COMMENT ON COLUMN jobswaiting4.avgrss IS E'AverageÂ RSS in the job';
COMMENT ON COLUMN jobswaiting4.avgswap IS E'Average SWAP in the job';
COMMENT ON COLUMN jobswaiting4.avgvmem IS E'Average VMEM in the job';
COMMENT ON COLUMN jobswaiting4.batchid IS E'ID of the job in the backend batch system';
COMMENT ON COLUMN jobswaiting4.brokerageerrorcode IS E'brokerage error code';
COMMENT ON COLUMN jobswaiting4.brokerageerrordiag IS E'brokerage error diagnostics';
COMMENT ON COLUMN jobswaiting4.cloud IS E'cloud (associated with Tier 1) where the job is submitted to';
COMMENT ON COLUMN jobswaiting4.cmtconfig IS E'cmt config';
COMMENT ON COLUMN jobswaiting4.commandtopilot IS E'used to send commands to pilot from Panda server, to kill the job for instance';
COMMENT ON COLUMN jobswaiting4.computingelement IS E'name of computing element';
COMMENT ON COLUMN jobswaiting4.computingsite IS E'site name where the job runs';
COMMENT ON COLUMN jobswaiting4.corecount IS E'the number of CPU cores';
COMMENT ON COLUMN jobswaiting4.countrygroup IS E'country of the job submitter';
COMMENT ON COLUMN jobswaiting4.cpuconsumptiontime IS E'actual execution time of the job';
COMMENT ON COLUMN jobswaiting4.cpuconsumptionunit IS E'unit for CPUCONSUMPTIONTIME';
COMMENT ON COLUMN jobswaiting4.cpuconversion IS E'CPU conversion factor';
COMMENT ON COLUMN jobswaiting4.creationhost IS E'the hostname where the job is submitted';
COMMENT ON COLUMN jobswaiting4.creationtime IS E'generated by Oracle''s SYSDATE function when the job is inserted to jobsDefined4';
COMMENT ON COLUMN jobswaiting4.currentpriority IS E'actual priority value which is usually the same as assignedPriority, can be modified by Panda server';
COMMENT ON COLUMN jobswaiting4.ddmerrorcode IS E'DDM error code';
COMMENT ON COLUMN jobswaiting4.ddmerrordiag IS E'DDM error diagnostics';
COMMENT ON COLUMN jobswaiting4.destinationdblock IS E'name of destination dataset for the job; is used to register the outputs of an associated set of jobs as belonging to one block to be saved at an archival destination';
COMMENT ON COLUMN jobswaiting4.destinationse IS E'destination storage element of job output files';
COMMENT ON COLUMN jobswaiting4.destinationsite IS E'destination site (usually SE) for file transfer';
COMMENT ON COLUMN jobswaiting4.diskio IS E'Local disk access in kBPerSec. Required to limit the number of running jobs based on total IO for each queue.';
COMMENT ON COLUMN jobswaiting4.dispatchdblock IS E'name of dispatch dataset for the job ; a prodDBlock may be broken down into smaller blocks for dispatch to sites';
COMMENT ON COLUMN jobswaiting4.endtime IS E'end time when the job finished on WN';
COMMENT ON COLUMN jobswaiting4.eventservice IS E'The job uses Event Service';
COMMENT ON COLUMN jobswaiting4.exeerrorcode IS E'executor error code';
COMMENT ON COLUMN jobswaiting4.exeerrordiag IS E'executor error diagnostics';
COMMENT ON COLUMN jobswaiting4.failedattempt IS E'How many times the input files were failed so far. The maximum number is used if there are multiple input files';
COMMENT ON COLUMN jobswaiting4.grid IS E'GRID where the job is submitted to';
COMMENT ON COLUMN jobswaiting4.gshare IS E'Global share';
COMMENT ON COLUMN jobswaiting4.homepackage IS E'Cache required to run the job';
COMMENT ON COLUMN jobswaiting4.hs06 IS E'Core count x core power';
COMMENT ON COLUMN jobswaiting4.hs06sec IS E'The product of HS06 score and CPU consumption time';
COMMENT ON COLUMN jobswaiting4.inputfilebytes IS E'the total size of input files';
COMMENT ON COLUMN jobswaiting4.inputfileproject IS E'project name of input files';
COMMENT ON COLUMN jobswaiting4.inputfiletype IS E'type of input files';
COMMENT ON COLUMN jobswaiting4.ipconnectivity IS E'defined in prodDB (unused in Panda)';
COMMENT ON COLUMN jobswaiting4.jobdefinitionid IS E'comes from ejobdef.jobdefid for managed production jobs, and defines job set IDs for analysis jobs';
COMMENT ON COLUMN jobswaiting4.jobdispatchererrorcode IS E'jobDispatcher error code';
COMMENT ON COLUMN jobswaiting4.jobdispatchererrordiag IS E'jobDispatcher error diagnostics';
COMMENT ON COLUMN jobswaiting4.jobexecutionid IS E'job execution ID coming from ProdDB';
COMMENT ON COLUMN jobswaiting4.jobmetrics IS E'a general-purpose field to record various job metrics';
COMMENT ON COLUMN jobswaiting4.jobname IS E'the job name defined in prodDB';
COMMENT ON COLUMN jobswaiting4.jobparameters IS E'unused. moved to ATLAS_PANDA.JOBPARAMSTABLE. remains for code compatibility';
COMMENT ON COLUMN jobswaiting4.jobsetid IS E'jobset ID for analysis jobs';
COMMENT ON COLUMN jobswaiting4.jobstatus IS E'status of the job';
COMMENT ON COLUMN jobswaiting4.lockedby IS E'the name of the current writer of the record in ProdDB';
COMMENT ON COLUMN jobswaiting4.maxattempt IS E'how many times the job can be retried';
COMMENT ON COLUMN jobswaiting4.maxcpucount IS E'expected execution time of the job';
COMMENT ON COLUMN jobswaiting4.maxcpuunit IS E'unit for MAXCPUCOUNT';
COMMENT ON COLUMN jobswaiting4.maxdiskcount IS E'expected disk size used by the job';
COMMENT ON COLUMN jobswaiting4.maxdiskunit IS E'unit for MAXDISKCOUNT';
COMMENT ON COLUMN jobswaiting4.maxpss IS E'Maximum PSS in the job';
COMMENT ON COLUMN jobswaiting4.maxrss IS E'Maximum RSS in the job';
COMMENT ON COLUMN jobswaiting4.maxswap IS E'Maximum SWAP in the job';
COMMENT ON COLUMN jobswaiting4.maxvmem IS E'Maximum VMEM in the job';
COMMENT ON COLUMN jobswaiting4.maxwalltime IS E'Estimated walltime limit for the job';
COMMENT ON COLUMN jobswaiting4.memory_leak IS E'Memory leak in KB/s';
COMMENT ON COLUMN jobswaiting4.memory_leak_x2 IS E'Memory leak square statistic';
COMMENT ON COLUMN jobswaiting4.metadata IS E'unused. moved to ATLAS_PANDA.METATABLE. remains for code compatibility';
COMMENT ON COLUMN jobswaiting4.minramcount IS E'expected amount of memory usage of the job';
COMMENT ON COLUMN jobswaiting4.minramunit IS E'unit for MINRAMCOUNT';
COMMENT ON COLUMN jobswaiting4.modificationhost IS E'the hostname which updates job status';
COMMENT ON COLUMN jobswaiting4.modificationtime IS E'timestamp in UTC set by Panda server when job state changes or pilot update is received';
COMMENT ON COLUMN jobswaiting4.nevents IS E'number of events processed by the job';
COMMENT ON COLUMN jobswaiting4.ninputdatafiles IS E'the number of input files';
COMMENT ON COLUMN jobswaiting4.ninputfiles IS E'the number of input files which the pilot processed on WN';
COMMENT ON COLUMN jobswaiting4.noutputdatafiles IS E'the number of output files';
COMMENT ON COLUMN jobswaiting4.nucleus IS E'Name of the site where the job is assigned in WORLD cloud';
COMMENT ON COLUMN jobswaiting4.outputfilebytes IS E'the total size of output files';
COMMENT ON COLUMN jobswaiting4.pandaid IS E'sequential ID generated from Oracle sequence object JOBSDEFINED4_PANDAID_SEQ when the job is inserted to jobsDefined4';
COMMENT ON COLUMN jobswaiting4.parentid IS E'when the job is a retry for another job, PandaID of the retried job is set in this field';
COMMENT ON COLUMN jobswaiting4.piloterrorcode IS E'pilot error code';
COMMENT ON COLUMN jobswaiting4.piloterrordiag IS E'pilot error diagnostics';
COMMENT ON COLUMN jobswaiting4.pilotid IS E'ID assigned to pilot by scheduler';
COMMENT ON COLUMN jobswaiting4.pilottiming IS E'time consumption of the pilot';
COMMENT ON COLUMN jobswaiting4.processingtype IS E'type of the job comes from etask.tasktype2';
COMMENT ON COLUMN jobswaiting4.proddblock IS E'name of dataset containing job input files';
COMMENT ON COLUMN jobswaiting4.proddbupdatetime IS E'timestamp of the last update in Oracle ProdDB';
COMMENT ON COLUMN jobswaiting4.prodserieslabel IS E'constrained to DC2, Rome, DC3, pandatest';
COMMENT ON COLUMN jobswaiting4.prodsourcelabel IS E'activity name of the name such as managed, user, and ddm';
COMMENT ON COLUMN jobswaiting4.produserid IS E'ID of the user defined the job (user''s certificateDN or e-mail address)';
COMMENT ON COLUMN jobswaiting4.produsername IS E'the name of the job submitter';
COMMENT ON COLUMN jobswaiting4.raterbytes IS E'Read bytes rate';
COMMENT ON COLUMN jobswaiting4.raterchar IS E'Read chars rate';
COMMENT ON COLUMN jobswaiting4.ratewbytes IS E'Write bytes rate';
COMMENT ON COLUMN jobswaiting4.ratewchar IS E'Write chars rate';
COMMENT ON COLUMN jobswaiting4.relocationflag IS E'flag for submitting jobs to a single site. I.e. the brokerage is bypassed';
COMMENT ON COLUMN jobswaiting4.resource_type IS E'Resource type name';
COMMENT ON COLUMN jobswaiting4.schedulerid IS E'ID identifying the pilot scheduler';
COMMENT ON COLUMN jobswaiting4.sourcesite IS E'source site (usually CE) for file transfer';
COMMENT ON COLUMN jobswaiting4.specialhandling IS E'set when the job is specially handled in PanDA, such as re-brokerage';
COMMENT ON COLUMN jobswaiting4.starttime IS E'start time when the job got started on WN';
COMMENT ON COLUMN jobswaiting4.statechangetime IS E'timestamp of the last state change';
COMMENT ON COLUMN jobswaiting4.superrorcode IS E'supervisor error code';
COMMENT ON COLUMN jobswaiting4.superrordiag IS E'supervisor error diagnostics';
COMMENT ON COLUMN jobswaiting4.taskbuffererrorcode IS E'taskBuffer error code';
COMMENT ON COLUMN jobswaiting4.taskbuffererrordiag IS E'taskBuffer error diagnostics';
COMMENT ON COLUMN jobswaiting4.taskid IS E'task ID defined in prodDB';
COMMENT ON COLUMN jobswaiting4.totrbytes IS E'Total read bytes';
COMMENT ON COLUMN jobswaiting4.totrchar IS E'Total read chars';
COMMENT ON COLUMN jobswaiting4.totwbytes IS E'Total write bytes';
COMMENT ON COLUMN jobswaiting4.totwchar IS E'Total write chars';
COMMENT ON COLUMN jobswaiting4.transexitcode IS E'transformation exit code';
COMMENT ON COLUMN jobswaiting4.transfertype IS E'type of file transfer';
COMMENT ON COLUMN jobswaiting4.transformation IS E'Payload job script';
COMMENT ON COLUMN jobswaiting4.vo IS E'Virtual Organization';
COMMENT ON COLUMN jobswaiting4.workinggroup IS E'working group name';
ALTER TABLE jobswaiting4 OWNER TO panda;
CREATE UNIQUE INDEX jobswaiting4_jeditaskid_idx ON jobswaiting4 (jeditaskid, pandaid);
CREATE INDEX jobswaiting4_jobexecid_idx ON jobswaiting4 (jobexecutionid);
CREATE INDEX jobswaiting4_reqid_idx ON jobswaiting4 (reqid);
CREATE INDEX jobswaiting4_workqueue_idx ON jobswaiting4 (workqueue_id, cloud, jobstatus, prodsourcelabel, currentpriority);
CREATE INDEX jobswait_statechangetime_idx ON jobswaiting4 (statechangetime);
ALTER TABLE jobswaiting4 ADD PRIMARY KEY (pandaid);

CREATE TABLE jobs_share_stats (
	ts timestamp,
	gshare varchar(32),
	computingsite varchar(128),
	jobstatus varchar(15) NOT NULL,
	maxpriority bigint,
	njobs bigint,
	hs bigint,
	vo varchar(16),
	workqueue_id integer,
	resource_type varchar(56),
	prorated_diskio_avg decimal(11,2)
) ;
COMMENT ON TABLE jobs_share_stats IS E'njobs and HS06 statistics by share for table jobsdefined4';
COMMENT ON COLUMN jobs_share_stats.computingsite IS E'Panda site';
COMMENT ON COLUMN jobs_share_stats.gshare IS E'Global share';
COMMENT ON COLUMN jobs_share_stats.hs IS E'HS06 = core count x core power x njob';
COMMENT ON COLUMN jobs_share_stats.jobstatus IS E'Jobstatus for the aggregation';
COMMENT ON COLUMN jobs_share_stats.maxpriority IS E'Max priority of share';
COMMENT ON COLUMN jobs_share_stats.njobs IS E'Number of jobs';
COMMENT ON COLUMN jobs_share_stats.prorated_diskio_avg IS E'Dummy entry, only for compatibility';
COMMENT ON COLUMN jobs_share_stats.resource_type IS E'Resource type (SCORE, MCORE...)';
COMMENT ON COLUMN jobs_share_stats.ts IS E'Timestamp for the entry';
COMMENT ON COLUMN jobs_share_stats.vo IS E'Virtual organization';
COMMENT ON COLUMN jobs_share_stats.workqueue_id IS E'Work queue';
ALTER TABLE jobs_share_stats OWNER TO panda;
CREATE INDEX jobssharestats_gsharestatusidx ON jobs_share_stats (gshare, jobstatus);
CREATE INDEX jobs_share_stats_compsite_idx ON jobs_share_stats (computingsite);

CREATE TABLE jobs_statuslog (
	pandaid bigint NOT NULL,
	modificationtime timestamp NOT NULL,
	jobstatus varchar(15) NOT NULL,
	prodsourcelabel varchar(20),
	cloud varchar(50),
	computingsite varchar(128),
	modificationhost varchar(128),
	modiftime_extended timestamp
) PARTITION BY RANGE (modificationtime) ;
COMMENT ON TABLE jobs_statuslog IS E'Logging table for hosting important job attributes whenever the job status changes(which are normally overwritten because of the updates in the JOBSACTIVE table. The agreed data retention is 2 months';
COMMENT ON COLUMN jobs_statuslog.cloud IS E'cloud (associated with Tier 1) where the job is submitted to';
COMMENT ON COLUMN jobs_statuslog.computingsite IS E'site name where the job runs';
COMMENT ON COLUMN jobs_statuslog.jobstatus IS E'status of the job ';
COMMENT ON COLUMN jobs_statuslog.modificationhost IS E'the hostname which updates job status';
COMMENT ON COLUMN jobs_statuslog.modificationtime IS E'timestamp (in UTC) set by Panda server when job state changes ';
COMMENT ON COLUMN jobs_statuslog.modiftime_extended IS E'Extended higher precision of the MODIFICATIONTIME column. The MODIFICATIONTIME column itself could not be changed from DATE to TIMESTAMP because of "ORA-14060: data type or length of a table partitioning column may not be changed" ';
COMMENT ON COLUMN jobs_statuslog.pandaid IS E'sequential ID generated from Oracle sequence object JOBSDEFINED4_PANDAID_SEQ when the job is inserted to jobsDefined4';
COMMENT ON COLUMN jobs_statuslog.prodsourcelabel IS E'activity name of the name such as managed, user, and ddm';
ALTER TABLE jobs_statuslog OWNER TO panda;
CREATE INDEX jobs_statuslog_pandaid_idx ON jobs_statuslog (pandaid);

CREATE TABLE job_output_report (
	pandaid bigint NOT NULL,
	prodsourcelabel varchar(20),
	jobstatus varchar(15) NOT NULL,
	attemptnr smallint NOT NULL,
	data text,
	timestamp timestamp NOT NULL,
	lockedby varchar(40),
	lockedtime timestamp
) ;
ALTER TABLE job_output_report OWNER TO panda;
ALTER TABLE job_output_report ADD PRIMARY KEY (pandaid,attemptnr);

CREATE TABLE job_stats_hp (
	ts timestamp,
	prodsourcelabel varchar(20),
	cloud varchar(50),
	resource_type varchar(56),
	gshare varchar(32),
	jobstatus varchar(15),
	workqueue_id integer,
	vo varchar(16),
	max_priority bigint,
	max_priority_count bigint
) ;
COMMENT ON TABLE job_stats_hp IS E'highest priority job statistics';
COMMENT ON COLUMN job_stats_hp.gshare IS E'Global share';
COMMENT ON COLUMN job_stats_hp.jobstatus IS E'Jobstatus for the aggregation';
COMMENT ON COLUMN job_stats_hp.max_priority IS E'Max priority of share';
COMMENT ON COLUMN job_stats_hp.max_priority_count IS E'Number of jobs with max priority';
COMMENT ON COLUMN job_stats_hp.resource_type IS E'Resource type (SCORE, MCORE...)';
COMMENT ON COLUMN job_stats_hp.ts IS E'Timestamp for the entry';
COMMENT ON COLUMN job_stats_hp.vo IS E'Virtual organization';
COMMENT ON COLUMN job_stats_hp.workqueue_id IS E'Work queue';
ALTER TABLE job_stats_hp OWNER TO panda;

CREATE TABLE lob_test (
	tex_i bigint,
	tex text
) ;
ALTER TABLE lob_test OWNER TO panda;

CREATE TABLE metatable (
	pandaid bigint NOT NULL,
	modificationtime timestamp NOT NULL DEFAULT to_date('01-JAN-1970 00:00:00','DD-MON-YYYY HH24:MI:SS'),
	metadata text
) PARTITION BY RANGE (modificationtime) ;
COMMENT ON TABLE metatable IS E'Table with information on the job output. When a PanDA job is in a defined or running state, relevant rows reside in the INITIAL partition of the table. When the job is finished or aborted the "modificationtime" is set the to real current time and as the table has "row movement" enabled, Oracle moves the rows from the INITIAL partition to the partitions of the current day. Data is regularly copied to an archive table in ATLAS_PANDAARCH schema. Data retention is defined to be 3 days (can be changed if necessary)';
COMMENT ON COLUMN metatable.metadata IS E'Meta data produced by the job in XML format. The column type is CLOB';
COMMENT ON COLUMN metatable.modificationtime IS E'modificationTime of the job (in UTC)';
COMMENT ON COLUMN metatable.pandaid IS E'PandaID of the job';
ALTER TABLE metatable OWNER TO panda;
ALTER TABLE metatable ADD PRIMARY KEY (pandaid,modificationtime);

CREATE TABLE mv_jobsactive4_stats (
	cur_date timestamp,
	cloud varchar(50),
	computingsite varchar(128),
	countrygroup varchar(20),
	workinggroup varchar(20),
	relocationflag smallint,
	jobstatus varchar(15) NOT NULL,
	processingtype varchar(64),
	prodsourcelabel varchar(20),
	currentpriority bigint,
	num_of_jobs bigint,
	vo varchar(16),
	workqueue_id integer
) ;
COMMENT ON TABLE mv_jobsactive4_stats IS E'Table (was from Materialized view before, but was not reliable) which collects aggregated data on set of attributes(columns). The data is read from the JOBSACTIVE4 table by an Oracle scheduler job. The refresh interval is 2 min';
COMMENT ON COLUMN mv_jobsactive4_stats.cloud IS E'cloud (associated with Tier 1) where the job is submitted to';
COMMENT ON COLUMN mv_jobsactive4_stats.computingsite IS E'site name where the job runs';
COMMENT ON COLUMN mv_jobsactive4_stats.countrygroup IS E'country of the job submitter';
COMMENT ON COLUMN mv_jobsactive4_stats.cur_date IS E'The timestamp of the Materialized view refresh ';
COMMENT ON COLUMN mv_jobsactive4_stats.currentpriority IS E'actual priority value which is usually the same as assignedPriority, can be modified by Panda server';
COMMENT ON COLUMN mv_jobsactive4_stats.jobstatus IS E'status of the job';
COMMENT ON COLUMN mv_jobsactive4_stats.num_of_jobs IS E'Number of jobs computed by grouping all set of attributes(columns) listed in that column,  ';
COMMENT ON COLUMN mv_jobsactive4_stats.processingtype IS E'type of the job comes from etask.tasktype2';
COMMENT ON COLUMN mv_jobsactive4_stats.prodsourcelabel IS E'activity name of the name such as managed, user, and ddm';
COMMENT ON COLUMN mv_jobsactive4_stats.relocationflag IS E'flag for submitting jobs to a single site. I.e. the brokerage is bypassed';
COMMENT ON COLUMN mv_jobsactive4_stats.workinggroup IS E'working group name';
ALTER TABLE mv_jobsactive4_stats OWNER TO panda;

CREATE TABLE network_matrix_kv (
	src varchar(256) NOT NULL,
	dst varchar(256) NOT NULL,
	key varchar(256) NOT NULL,
	value bigint,
	ts timestamp
) ;
COMMENT ON TABLE network_matrix_kv IS E'Network matrix based on key-value columns to hold arbitrary metrics';
COMMENT ON COLUMN network_matrix_kv.dst IS E'Destination site';
COMMENT ON COLUMN network_matrix_kv.key IS E'Metric key';
COMMENT ON COLUMN network_matrix_kv.src IS E'Source site';
COMMENT ON COLUMN network_matrix_kv.ts IS E'Timestamp for the entry';
COMMENT ON COLUMN network_matrix_kv.value IS E'Metric value';
ALTER TABLE network_matrix_kv OWNER TO panda;
ALTER TABLE network_matrix_kv ADD PRIMARY KEY (src,dst,key);

CREATE TABLE pandalog (
	bintime timestamp NOT NULL DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	name varchar(30),
	module varchar(30),
	loguser varchar(80),
	type varchar(20),
	pid bigint NOT NULL DEFAULT '0',
	loglevel integer NOT NULL DEFAULT '0',
	levelname varchar(30),
	time varchar(30),
	filename varchar(100),
	line integer NOT NULL DEFAULT '0',
	message varchar(4000)
) PARTITION BY RANGE (bintime) ;
COMMENT ON TABLE pandalog IS E'Table with job logging information which contains log messages from various applications';
COMMENT ON COLUMN pandalog.bintime IS E'set when the message is inserted';
COMMENT ON COLUMN pandalog.filename IS E'file name from which the message is sent';
COMMENT ON COLUMN pandalog.levelname IS E'name of verbosity level';
COMMENT ON COLUMN pandalog.line IS E'the number of lines in the message';
COMMENT ON COLUMN pandalog.loglevel IS E'verbosity level of the message';
COMMENT ON COLUMN pandalog.loguser IS E'user name who sends the message';
COMMENT ON COLUMN pandalog.message IS E'message content';
COMMENT ON COLUMN pandalog.module IS E'module name in the application';
COMMENT ON COLUMN pandalog.name IS E'application name';
COMMENT ON COLUMN pandalog.pid IS E'process identifier which sends the message';
COMMENT ON COLUMN pandalog.time IS E'set when the message is sent from the application';
COMMENT ON COLUMN pandalog.type IS E'message type';
ALTER TABLE pandalog OWNER TO panda;
CREATE INDEX pandalog_bintime_indx ON pandalog (bintime, type);
CREATE INDEX pandalog_multicolumn_indx ON pandalog (bintime, name, type, levelname);
CREATE INDEX pandalog_nametypebintime_indx ON pandalog (type, name, bintime desc);

CREATE TABLE pandalog_fax (
	bintime timestamp NOT NULL DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	name varchar(30),
	module varchar(30),
	loguser varchar(80),
	type varchar(20),
	pid bigint DEFAULT '0',
	loglevel integer DEFAULT '0',
	levelname varchar(30),
	time varchar(30),
	filename varchar(100),
	line integer DEFAULT '0',
	message varchar(4000)
) PARTITION BY RANGE (bintime) ;
ALTER TABLE pandalog_fax OWNER TO panda;

CREATE TABLE panda_ddm_relation (
	panda_site_name varchar(52) NOT NULL,
	ddm_endpoint_name varchar(52) NOT NULL,
	is_local char(1),
	roles varchar(60),
	order_read integer,
	order_write integer,
	default_read char(1),
	default_write char(1),
	scope varchar(52) NOT NULL
) ;
COMMENT ON TABLE panda_ddm_relation IS E'Table to handle the m to n relationships between PanDA sites and DDM endpoints.Table needed for the "Configurator" agent. The table consolidate information from other sources (AGIS and Rucio ) and caches it so that Panda can easily retrieve it and use it for brokerage.';
COMMENT ON COLUMN panda_ddm_relation.ddm_endpoint_name IS E'DDM endpoint name';
COMMENT ON COLUMN panda_ddm_relation.default_read IS E'Marks the default DDM endpoint for reading';
COMMENT ON COLUMN panda_ddm_relation.default_write IS E'Marks the default DDM endpoint for writing';
COMMENT ON COLUMN panda_ddm_relation.is_local IS E'Defines whether the CPU is local to the storage. E.g. an HPC center reading from BNL is not local. (Y/N)';
COMMENT ON COLUMN panda_ddm_relation.order_read IS E'DDM endpoint order for reading';
COMMENT ON COLUMN panda_ddm_relation.order_write IS E'DDM endpoint order for writing';
COMMENT ON COLUMN panda_ddm_relation.panda_site_name IS E'PanDA site name';
COMMENT ON COLUMN panda_ddm_relation.roles IS E'How the panda site can use the ddm endpoint (read, write,â¦)';
ALTER TABLE panda_ddm_relation OWNER TO panda;
CREATE INDEX pan_ddm_rel_endpoint_rel_idx ON panda_ddm_relation (ddm_endpoint_name);
CREATE INDEX pan_ddm_rel_pandasitename_idx ON panda_ddm_relation (panda_site_name);
ALTER TABLE panda_ddm_relation ADD PRIMARY KEY (panda_site_name,ddm_endpoint_name,scope);
ALTER TABLE panda_ddm_relation ADD CONSTRAINT panda_ddm_rel_is_local_check CHECK ( IS_LOCAL IN ('Y', 'N'));

CREATE TABLE panda_site (
	panda_site_name varchar(52) NOT NULL,
	site_name varchar(52) NOT NULL,
	storage_site_name varchar(52),
	default_ddm_endpoint varchar(52),
	is_local char(1)
) ;
COMMENT ON TABLE panda_site IS E'PanDA site/queue. Table needed for the "Configurator" agent. The table consolidate information from other sources (AGIS and Rucio ) and caches it so that Panda can easily retrieve it and use it for brokerage.';
COMMENT ON COLUMN panda_site.default_ddm_endpoint IS E'Defines the default DDM endpoint';
COMMENT ON COLUMN panda_site.is_local IS E'Defines whether the panda site is local to the storage. Can be remote in case of HPC, cloud, etc.';
COMMENT ON COLUMN panda_site.panda_site_name IS E'PanDA site name';
COMMENT ON COLUMN panda_site.site_name IS E'Site name';
COMMENT ON COLUMN panda_site.storage_site_name IS E'Defines the site used for storage';
ALTER TABLE panda_site OWNER TO panda;
CREATE INDEX panda_site_def_endpoint_idx ON panda_site (default_ddm_endpoint);
CREATE INDEX panda_site_site_name_idx ON panda_site (site_name);
CREATE INDEX panda_site_storage_name_idx ON panda_site (storage_site_name);
ALTER TABLE panda_site ADD PRIMARY KEY (panda_site_name);
ALTER TABLE panda_site ADD CONSTRAINT panda_site_is_local_check CHECK ( IS_LOCAL IN ('Y', 'N'));

CREATE TABLE pilottoken (
	token varchar(64) NOT NULL,
	schedulerhost varchar(100),
	scheduleruser varchar(150),
	usages integer NOT NULL DEFAULT '1',
	created timestamp NOT NULL DEFAULT LOCALTIMESTAMP,
	expires timestamp NOT NULL DEFAULT to_date('01-JAN-70 00:00:00','dd-MON-yy hh24:mi:ss'),
	schedulerid varchar(80)
) ;
ALTER TABLE pilottoken OWNER TO panda;
ALTER TABLE pilottoken ADD PRIMARY KEY (token);

CREATE TABLE resource_types (
	resource_name varchar(56) NOT NULL,
	mincore integer,
	maxcore integer,
	minrampercore integer,
	maxrampercore integer
) ;
COMMENT ON TABLE resource_types IS E'Types of resources, e.g. SCORE, MCORE, SCORE_HIMEM, MCORE_HIMEM';
COMMENT ON COLUMN resource_types.maxcore IS E'Maximum number of cores';
COMMENT ON COLUMN resource_types.maxrampercore IS E'Maximum RAM';
COMMENT ON COLUMN resource_types.mincore IS E'Minimum number of cores';
COMMENT ON COLUMN resource_types.minrampercore IS E'Minimum RAM';
COMMENT ON COLUMN resource_types.resource_name IS E'Resource type name';
ALTER TABLE resource_types OWNER TO panda;
ALTER TABLE resource_types ADD PRIMARY KEY (resource_name);

CREATE TABLE retryactions (
	retryaction_id bigint NOT NULL,
	retry_action varchar(50) NOT NULL,
	active char(1) NOT NULL DEFAULT 'Y',
	retry_description varchar(250)
) ;
ALTER TABLE retryactions OWNER TO panda;
ALTER TABLE retryactions ADD PRIMARY KEY (retryaction_id);

CREATE TABLE retryerrors (
	retryerror_id bigint NOT NULL,
	errorsource varchar(256) NOT NULL,
	errorcode bigint NOT NULL,
	active char(1) NOT NULL DEFAULT 'Y',
	retryaction bigint NOT NULL,
	errordiag varchar(256),
	parameters varchar(256),
	architecture varchar(256),
	release varchar(64),
	workqueue_id integer,
	description varchar(250),
	expiration_date timestamp
) ;
ALTER TABLE retryerrors OWNER TO panda;
CREATE INDEX retryerrors_retryaction_idx ON retryerrors (retryaction);
ALTER TABLE retryerrors ADD PRIMARY KEY (retryerror_id);

CREATE TABLE schedconfig_json (
	panda_queue varchar(50) NOT NULL,
	data text,
	last_update timestamp
) ;
COMMENT ON TABLE schedconfig_json IS E'Table to store the AGIS''s JSON configuration for each panda queue';
COMMENT ON COLUMN schedconfig_json.data IS E'Configuration downloaded from the AGIS/CRIC schedconfig file and stored in JSON block';
COMMENT ON COLUMN schedconfig_json.last_update IS E'Last time the PanDA queue was seen/updated';
COMMENT ON COLUMN schedconfig_json.panda_queue IS E'PanDA queue name';
ALTER TABLE schedconfig_json OWNER TO panda;
ALTER TABLE schedconfig_json ADD PRIMARY KEY (panda_queue);

CREATE TABLE site (
	site_name varchar(52) NOT NULL,
	role varchar(256),
	tier_level smallint,
	state varchar(52)
) ;
COMMENT ON TABLE site IS E'AGIS site, containing DDM endpoints and PanDA sites';
COMMENT ON COLUMN site.role IS E'Role:nucleus/satelite. Nuclei can get tasks assigned and will be the final destination of the task output. Satelites will provide compute power to the task.';
COMMENT ON COLUMN site.site_name IS E'Site name';
COMMENT ON COLUMN site.state IS E'Defines the state of a site';
COMMENT ON COLUMN site.tier_level IS E'Tier level (0/1/2/3) of the site';
ALTER TABLE site OWNER TO panda;
ALTER TABLE site ADD PRIMARY KEY (site_name);

CREATE TABLE site_stats (
	site_name varchar(52) NOT NULL,
	key varchar(52) NOT NULL,
	value bigint,
	ts timestamp
) ;
COMMENT ON TABLE site_stats IS E'Key-value table to hold metrics about a site';
COMMENT ON COLUMN site_stats.key IS E'Metric key';
COMMENT ON COLUMN site_stats.site_name IS E'Site name';
COMMENT ON COLUMN site_stats.ts IS E'Timestamp for the entry';
COMMENT ON COLUMN site_stats.value IS E'Metric value';
ALTER TABLE site_stats OWNER TO panda;
ALTER TABLE site_stats ADD PRIMARY KEY (site_name,key);

CREATE TABLE tablepart4copying (
	table_name varchar(30) NOT NULL,
	partition_name varchar(30) NOT NULL,
	copied_to_arch varchar(10) NOT NULL,
	copying_done_on timestamp,
	deleted_on timestamp,
	data_verif_passed char(3),
	data_verified_on timestamp
) PARTITION BY LIST (copied_to_arch) ;
COMMENT ON TABLE tablepart4copying IS E'Table for logging information on the process of data copying from the PANDA to PANDAARCH schema and logging on the partition removal for sustaining certain sliding window in the PANDA schema';
COMMENT ON COLUMN tablepart4copying.copied_to_arch IS E'Flag identifying "Y" or "N" the partition data has been copied to ATLAS_PANDAARCH';
COMMENT ON COLUMN tablepart4copying.copying_done_on IS E'Timestamp of the copying';
COMMENT ON COLUMN tablepart4copying.data_verif_passed IS E'Data verification passed "YES" or "NO" (PANDAIDs comparison) before partition removal';
COMMENT ON COLUMN tablepart4copying.data_verified_on IS E'Timestamp of the verification.';
COMMENT ON COLUMN tablepart4copying.deleted_on IS E'Date of the partition deletion ';
COMMENT ON COLUMN tablepart4copying.partition_name IS E'The partition name of the table';
COMMENT ON COLUMN tablepart4copying.table_name IS E'The name of the table which data is considered for copying';
ALTER TABLE tablepart4copying OWNER TO panda;
ALTER TABLE tablepart4copying ADD PRIMARY KEY (table_name,partition_name,copied_to_arch);

CREATE TABLE tasks_statuslog (
	jeditaskid bigint NOT NULL,
	modificationtime timestamp NOT NULL,
	status varchar(64) NOT NULL,
	modificationhost varchar(128),
	attemptnr smallint,
	reason varchar(255)
) PARTITION BY RANGE (modificationtime) ;
COMMENT ON TABLE tasks_statuslog IS E'Table to track status changes for tasks';
ALTER TABLE tasks_statuslog OWNER TO panda;
CREATE INDEX task_statuslog_jeditaskid_idx ON tasks_statuslog (jeditaskid);

CREATE TABLE tmp_pandaids_relations (
	pandaid bigint NOT NULL,
	newjobid bigint NOT NULL
) ;
ALTER TABLE tmp_pandaids_relations OWNER TO panda;

CREATE TABLE total_walltime_cache (
	vo varchar(16) NOT NULL,
	agg_type varchar(16) NOT NULL,
	agg_key varchar(32) NOT NULL,
	prodsourcelabel varchar(20) NOT NULL,
	resource_type varchar(56) NOT NULL,
	total_walltime bigint,
	n_has_value integer,
	n_no_value integer
) ;
ALTER TABLE total_walltime_cache OWNER TO panda;
ALTER TABLE total_walltime_cache ADD PRIMARY KEY (vo,agg_type,agg_key,prodsourcelabel,resource_type);

CREATE TABLE typical_num_input (
	vo varchar(16) NOT NULL,
	agg_type varchar(16) NOT NULL,
	agg_key varchar(32) NOT NULL,
	prodsourcelabel varchar(20) NOT NULL,
	processingtype varchar(64) NOT NULL,
	ninputdatafiles integer
) ;
COMMENT ON TABLE typical_num_input IS E'Cache for queued walltime aggregations';
ALTER TABLE typical_num_input OWNER TO panda;
ALTER TABLE typical_num_input ADD PRIMARY KEY (vo,agg_type,agg_key,prodsourcelabel,processingtype);

CREATE TABLE ups_stats (
	ts timestamp,
	computingsite varchar(128),
	resource_type varchar(56),
	gshare varchar(32),
	jobstatus varchar(15),
	vo varchar(16),
	current_priority_binned bigint,
	current_priority_count bigint
) ;
COMMENT ON TABLE ups_stats IS E'activated job statistics for UPS binned by priority ranges';
COMMENT ON COLUMN ups_stats.computingsite IS E'PanDA queue';
COMMENT ON COLUMN ups_stats.current_priority_binned IS E'Max priority of share';
COMMENT ON COLUMN ups_stats.current_priority_count IS E'Number of jobs with max priority';
COMMENT ON COLUMN ups_stats.gshare IS E'Global share';
COMMENT ON COLUMN ups_stats.jobstatus IS E'Jobstatus for the aggregation';
COMMENT ON COLUMN ups_stats.resource_type IS E'Resource type (SCORE, MCORE...)';
COMMENT ON COLUMN ups_stats.ts IS E'Timestamp for the entry';
COMMENT ON COLUMN ups_stats.vo IS E'Virtual organization';
ALTER TABLE ups_stats OWNER TO panda;

CREATE TABLE network_matrix_kv_temp (
	src varchar(256) NOT NULL,
	dst varchar(256) NOT NULL,
	key varchar(256) NOT NULL,
	value bigint,
    ts timestamp
) ;
ALTER TABLE network_matrix_kv_temp OWNER TO panda;
